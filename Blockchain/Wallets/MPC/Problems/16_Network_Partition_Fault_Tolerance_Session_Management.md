# Network Partition Fault Tolerance and Session Management in MPC Wallets

**Last Updated**: 2025-11-29  
**Status**: Draft  
**Owner**: Distributed Systems & Reliability Team

## Problem Statement

1. **[Important]** Q: MPC wallet systems exhibit fragile availability under network partition conditions, where distributed signing parties (typically 2-of-2, 2-of-3, or 3-of-5 configurations serving 100M+ users managing $50B+ assets) lose network connectivity during signing sessions due to transient failures (cloud provider outages, ISP disruptions, firewall misconfigurations, DDoS attacks), resulting in incomplete signing rounds that block transactions indefinitely until manual intervention (average 2-6h recovery time causing $50K-$500K opportunity cost for institutional trading operations [Estimated from exchange downtime impact]), yet current implementations lack robust fault tolerance mechanisms: Byzantine Fault Tolerance (BFT) threshold choice optimizes for malicious adversary resistance (requiring ≥(2t+1) parties for t-of-n threshold tolerating t Byzantine faults [NIST: BFT Consensus, 2023]) but sacrifices liveness under crash failures (network partitions behave as crash faults, 2-of-3 becomes unavailable if 1 party unreachable despite no malicious behavior), session timeout configurations inadequately balance liveness vs. safety (short timeouts <30s enable fast failure detection but false-positive abort during transient network delays, long timeouts >5min improve robustness but delay transaction completion), split-brain scenarios where partitioned subsystems both believe themselves quorum risk double-signing (multiple signing sessions proceeding concurrently with different party subsets, potentially creating conflicting signatures violating wallet security guarantees), and recovery protocols remain ad-hoc (manual session restart, party health checks, state reconciliation unspecified in most implementations [Market observation, 2025]). Formulate a structured problem statement using the following [Input] fields.
   
   A:
   - **Brief description of the problem to be analyzed**: 
     MPC wallets operate as distributed systems where signing requires coordinated multi-party computation (2-of-2, 2-of-3, 3-of-5 threshold configurations common [Market observation]). Network partitions break coordination: (1) Liveness failures where threshold signature generation requires t-of-n parties to complete protocol rounds (key generation round 1-5, signing rounds 1-3 depending on protocol), network partition isolating ≥(n-t+1) parties blocks transaction signing indefinitely (2-of-3 wallet becomes unavailable if 2 parties partitioned from 1, cannot reach threshold despite majority available); (2) Split-brain risk where partitioned subsystems independently proceed with signing (Subsystem A: parties 1+2, Subsystem B: party 3, both believe quorum if misconfigured), potentially generating conflicting signatures or double-spending; (3) Session management complexity where signing protocols maintain ephemeral state (commitment, zero-knowledge proofs, Paillier encryptions), network interruption mid-round leaves inconsistent state (party 1 completed round 2, party 2 aborted after round 1, party 3 never started), requiring state reconciliation or full session restart; (4) Timeout sensitivity where MPC rounds involve network communication (broadcast commitments, await responses), timeout configuration trades off liveness (short timeout <30s detects failures fast but aborts on transient delays, long timeout >5min tolerates latency spikes but delays legitimate failure detection); (5) CAP theorem trade-off where distributed systems under partition must choose consistency or availability (MPC wallets prioritize safety—no double-signing—over liveness, transactions block rather than risk conflicting signatures, institutional users prefer availability for time-sensitive trading [Stakeholder preference]); (6) Recovery protocol gaps where most implementations lack specified procedures for post-partition reconciliation (manual intervention common: restart signing, verify party health, clear stale sessions, average 2-6h operator response time [Estimated from incident reports]).
   
   - **Background and current situation**: 
     Distributed systems fundamentals: Byzantine Fault Tolerance (BFT) protocols tolerate malicious adversaries presenting different behaviors to different parties [NIST: BFT Consensus, 2023]. Classical BFT threshold: ≥(2t+1) total parties required to tolerate t Byzantine faults (e.g., 3-of-5 tolerates 2 Byzantine, 2-of-3 tolerates 1 Byzantine). Network partitions behave as crash faults (nodes unreachable, not malicious), typically requiring only ≥(t+1) parties for t crash fault tolerance. MPC threshold signatures: t-of-n threshold means ≥t parties must collaborate to generate signature. Common configurations: 2-of-2 (user device + provider server, ZenGo model, maximizes security but zero fault tolerance—either party failure blocks signing), 2-of-3 (user + provider + backup, Coinbase WaaS model, tolerates 1 party unavailability), 3-of-5 (institutional custody, Fireblocks model, tolerates 2 party unavailability). MPC signing protocol rounds: ECDSA threshold signatures (GG20, CGGMP21) require 3-5 protocol rounds depending on variant [Paper: CGGMP21, 2021], each round involves: (1) Broadcast commitment, (2) Await responses from ≥t parties, (3) Verify zero-knowledge proofs, (4) Proceed to next round or abort. Round timeouts: Most implementations use 30s-5min timeouts per round [Market observation], total signing latency 2-10s typical under normal conditions (no failures), 30s-5min under transient delays, indefinite under persistent partition. Network partition causes: Cloud provider outages (AWS, GCP, Azure regional failures, 99.9% SLA implies 8.7h/year downtime, institutional MPC wallets often deploy across multiple clouds but partitions still occur [Cloud SLA data]); ISP disruptions (BGP hijacking, fiber cuts, DDoS attacks affecting connectivity between parties); Firewall/security misconfigurations (corporate firewalls blocking MPC protocol ports, security policy changes breaking connectivity); Geographic distance (cross-region latency spikes, undersea cable disruptions for international deployments). Split-brain scenarios: Partitioned subsystem A (parties 1+2) and subsystem B (party 3) both believe themselves quorum if threshold misconfigured or partition detection fails, risk double-signing (A signs transaction T1, B signs conflicting transaction T2). Prevention requires quorum enforcement (verify ≥t parties reachable before starting signature, abort if partition detected mid-signing). CAP theorem: Network partition forces choice between Consistency (all parties see same state, no conflicting signatures) and Availability (system continues operating despite partition) [Distributed systems theory]. MPC wallets prioritize consistency (safety over liveness): transactions block rather than risk double-signing, users tolerate unavailability (2-6h downtime) over security compromise. Current implementations: Most MPC wallet providers lack documented fault tolerance specifications (Fireblocks, Coinbase WaaS, ZenGo, BitGo do not publish detailed network partition handling [Market observation, 2025]). Industry practice: Manual recovery (operators restart sessions, verify party health, clear stale state), average 2-6h response time [Estimated from incident reports]. Research gap: Academic MPC protocols focus on malicious adversary security, not practical crash fault tolerance; few papers address network partition recovery, session management, timeout optimization for production MPC wallets.
   
   - **Goals and success criteria**: 
     Availability improvement: Reduce transaction unavailability during network partitions → <1% of signing requests blocked by transient network failures by Q4 2027 (current: est. 5-10% institutional users experience partition-related delays monthly [Estimated from support tickets]). Fault tolerance: Achieve crash fault tolerance matching threshold (2-of-3 wallet tolerates 1 party crash, 3-of-5 tolerates 2 party crashes) → 99.9% availability for 2-of-3 configurations (8.7h/year downtime), 99.99% for 3-of-5 (52min/year downtime) by Q4 2027. Split-brain prevention: Zero double-signing incidents due to network partition (current: no public incidents reported, but risk unquantified) → maintain 100% safety guarantee. Session recovery time: Reduce average recovery time from partition → <5min automated recovery (vs. current 2-6h manual intervention) for 90% of transient partitions by Q4 2026. Timeout optimization: Dynamic timeout adjustment → adapt timeout thresholds based on observed network conditions (30s baseline, extend to 5min under high latency, abort at 10min hard limit), reduce false-positive aborts by 80% (vs. static timeout causing unnecessary session restarts) by Q4 2026. Partition detection latency: <10s detection of network partition (party unreachable) → trigger failover or session abort promptly vs. waiting full timeout period, improve user experience (explicit failure message vs. hanging indefinitely) by Q4 2026. State reconciliation: Automated session cleanup → 100% of aborted sessions cleaned up within 1min (clear stale commitments, release locks, mark session failed in state machine), prevent state corruption requiring manual intervention by Q4 2026. Monitoring & alerting: Real-time partition detection → ≥3 major providers implementing distributed system health monitoring (party reachability, round completion rates, timeout frequency, partition duration) with automated alerting by Q4 2027. Documentation: 100% of major providers publishing network partition handling specifications (fault tolerance guarantees, timeout configurations, recovery procedures, expected downtime SLAs) by Q4 2027 (current: 0% public documentation [Market observation]).
   
   - **Key constraints and resources**: 
     Timeline: Q1 2026 - Q4 2027 (24 months for research, protocol enhancements, implementation, testing, production rollout, monitoring infrastructure); Critical path: Fault-tolerant protocol design (Q1-Q2 2026) → implementation + testing (Q3-Q4 2026) → production deployment (Q1-Q2 2027) → monitoring + refinement (Q3-Q4 2027). Budget: $2M-$10M per major provider (distributed systems research $500K-$2M, protocol enhancement $1M-$4M implementing robust session management + partition detection + automated recovery, testing infrastructure $500K-$2M simulating network partitions + Byzantine faults, monitoring/alerting $500K-$1M real-time health dashboards, documentation $200K-$500K SLA specifications + runbooks); Total ecosystem $30M-$150M (15+ providers). Team: Per provider requires 2-4 distributed systems engineers (fault tolerance protocol design, session management state machines, recovery procedures), 3-5 senior backend engineers (implementation, integration with existing MPC signing, testing), 1-2 SRE/DevOps engineers (monitoring infrastructure, alerting, incident response automation), 1-2 QA engineers (chaos engineering, partition simulation, fault injection testing), 1 technical writer (documentation, SLA specifications, user-facing communication). Protocol constraints: BFT threshold choice (≥(2t+1) for t Byzantine faults vs. ≥(t+1) for t crash faults, MPC wallets use t-of-n threshold signatures where t chosen balancing security and availability, cannot reduce t without weakening security assumptions); MPC round synchrony (protocols assume semi-synchronous network with bounded delays, asynchronous MPC more complex and slower [Research: asynchronous MPC, limited production implementations]); State consistency (all parties must agree on session state—aborted, completed, or in-progress—to prevent split-brain, requires distributed consensus adding overhead). Performance constraints: Timeout duration affects user experience (short timeout <30s fails fast but aborts on transient delays, long timeout >5min tolerates delays but frustrates users waiting for hung sessions); Partition detection latency (heartbeat frequency trades off overhead vs. detection speed, 1s heartbeats enable <10s detection but increase network traffic, 30s heartbeats slower detection but lower overhead); Recovery overhead (state reconciliation, session restart requires re-executing rounds, adds latency penalty 2-5s for automated recovery vs. indefinite hang without recovery). Deployment constraints: Backward compatibility (new fault-tolerant protocols must interoperate with existing deployed wallets, phased rollout necessary); Multi-cloud complexity (institutional deployments span AWS + GCP + Azure for redundancy, partition simulation must test all failure modes, cross-cloud latency 50-200ms baseline increases timeout requirements). Testing constraints: Chaos engineering (systematically injecting network partitions, delays, packet loss to validate fault tolerance, requires dedicated test infrastructure); Reproducing rare failure modes (split-brain scenarios infrequent in production, must proactively test via fault injection rather than waiting for real incidents).
   
   - **Stakeholders and roles**: 
     Institutional Users (managing $30B+ high-frequency trading operations, need 99.9%+ availability for time-sensitive transactions, constraint: cannot tolerate 2-6h manual recovery delays causing $50K-$500K opportunity cost per incident [Estimated]); Retail Users (70M+ individuals, need seamless experience without hanging transactions, constraint: lack technical understanding of network partitions, expect instant feedback on transaction failure vs. indefinite hang); MPC Wallet Providers (15+ companies, need robust fault tolerance to meet enterprise SLAs, constraint: $2M-$10M per-provider implementation cost + backward compatibility with deployed wallets + research gap in asynchronous MPC); Site Reliability Engineers (SRE teams managing MPC wallet infrastructure, need automated recovery reducing on-call burden, constraint: current manual intervention 2-6h average response time outside business hours, 30min during business hours); Distributed Systems Researchers (academic institutions, need practical fault tolerance problems bridging theory and production, constraint: MPC protocol research typically assumes malicious adversaries not crash faults/partitions, limited collaboration with industry); Cloud Providers (AWS, GCP, Azure, provide underlying infrastructure with 99.9% SLA, constraint: cannot eliminate network partitions entirely, regional failures occur despite redundancy); Network Security Teams (corporate IT managing firewalls, VPNs, DDoS protection, need clear MPC protocol requirements, constraint: security policies may inadvertently block MPC traffic causing partitions); Regulatory Compliance (financial authorities requiring custody availability, need documented SLAs, constraint: unclear whether 99% vs. 99.9% vs. 99.99% availability threshold appropriate for digital asset custody); End-User Applications (DeFi protocols, exchanges, payment apps integrating MPC wallets, need predictable failure modes, constraint: hanging transactions vs. explicit failures affect user experience, timeout tuning complex).
   
   - **Time scale and impact scope**: 
     Timeline: Q1 2026 - Q4 2027 (24 months for fault-tolerant protocol design, implementation, production deployment, monitoring infrastructure); Phases: (1) Q1-Q2 2026: Research + protocol design (distributed systems fault tolerance for MPC threshold signatures, session management state machines, partition detection algorithms); (2) Q3-Q4 2026: Implementation + testing (protocol enhancements, chaos engineering validation, partition simulation); (3) Q1-Q2 2027: Production deployment (phased rollout, backward compatibility with existing wallets, monitoring infrastructure); (4) Q3-Q4 2027: Refinement (timeout optimization based on real-world data, automated recovery tuning, documentation). Systems: MPC signing protocols (ECDSA threshold GG20/CGGMP21, EdDSA threshold, protocol round synchronization, timeout configurations, abort conditions), session management (state machines tracking signing progress: initiated → round 1 → round 2 → ... → completed/aborted, stale session cleanup, state reconciliation after partition), partition detection (heartbeat mechanisms between parties, reachability testing, latency monitoring, partition vs. slow-network disambiguation), recovery procedures (automated session restart, party health verification, state rollback to last consistent checkpoint, user notification of failure), distributed consensus (quorum enforcement preventing split-brain, distributed locking ensuring single active session per wallet, leader election for coordinator role), fault injection testing (chaos engineering infrastructure, network partition simulation, latency injection, packet loss, Byzantine behavior), monitoring & alerting (real-time dashboards: party reachability, signing success rate, timeout frequency, partition duration, automated alerts for prolonged unavailability), SLA specification (documented availability targets 99%, 99.9%, 99.99% for different wallet tiers, expected recovery time objectives RTO <5min, recovery point objectives RPO zero data loss); Current state: Majority of providers lack documented fault tolerance (no public specifications for partition handling [Market observation, 2025]), manual recovery standard (2-6h operator intervention [Estimated]), research gap (asynchronous MPC limited production implementations); Scale: 100M+ users affected by network partition unavailability, $50B+ assets at risk during prolonged outages, 15+ providers requiring implementation, $30M-$150M total ecosystem investment; Impact: Improved availability (99.9%+ uptime reduces institutional opportunity cost $50K-$500K per incident), automated recovery (<5min vs. 2-6h manual), better user experience (explicit failure messages vs. indefinite hangs), enhanced trust (documented SLAs + monitoring transparency), competitive differentiation (providers with robust fault tolerance attract institutional customers).
   
   - **Historical attempts and existing solutions (if any)**: 
     2010s: Classical distributed systems research established fault tolerance fundamentals: Byzantine Fault Tolerance (BFT) requires ≥(2t+1) parties for t Byzantine faults [NIST: BFT Consensus, 2023], Crash Fault Tolerance (CFT) requires ≥(t+1) parties for t crash faults. Consensus protocols (Paxos, Raft) provide strong consistency under partitions via majority quorum. CAP theorem (2000, Eric Brewer) formalizes consistency-availability trade-off under network partitions [Distributed systems theory]. Key lesson: Distributed systems must explicitly choose safety (consistency) or liveness (availability) during partitions; MPC wallets implicitly prioritize safety (no double-signing) over availability. 2016-2020: MPC threshold signature protocols (GG18, GG20, CGGMP21) developed focusing on malicious adversary security, not crash faults or network partitions [Paper: CGGMP21, 2021]. Protocols assume semi-synchronous network (bounded delays), timeout configurations unspecified, partition handling left to implementation. Key lesson: Academic protocols prioritize cryptographic security over operational robustness; production implementations must bridge gap. 2018-2023: Cloud-based MPC wallet deployments (Fireblocks 2-of-2 user+server, Coinbase WaaS 2-of-3, BitGo 3-of-5 institutional) experienced network partition incidents (AWS regional outages, cross-cloud latency spikes, firewall misconfigurations [Anecdotal from provider support channels]). Recovery: Manual intervention (SRE restart sessions, verify party health, clear stale state), average 2-6h response time [Estimated from incident reports]. Key lesson: Lack of automated recovery imposes high operational burden; institutional users tolerate unavailability if communicated transparently vs. silent hangs frustrating users. 2020-2022: Chaos engineering practices (Netflix Chaos Monkey, Amazon Game Days) demonstrated value of proactive fault injection testing [Industry practice]. Applied to MPC wallets: Systematically partition parties, inject latency, drop packets to validate fault tolerance before production incidents. Key lesson: Testing reveals brittle assumptions (e.g., hard-coded 30s timeout aborts during cross-region latency spikes, split-brain risk if quorum not enforced). No public MPC wallet providers publish chaos testing results. 2023-2025: Asynchronous MPC research explores removing synchrony assumptions (protocols tolerating arbitrary message delays, no timeouts) [Research: asynchronous MPC]. Promising for partition robustness but performance overhead significant (5-10× slower than semi-synchronous MPC [Research estimates]), limited production implementations. Key lesson: Asynchronous MPC theoretically optimal but impractical performance trade-off; hybrid approaches (semi-synchronous with adaptive timeouts) more viable. Session management state machines: Database literature (two-phase commit, three-phase commit protocols) addresses distributed transaction consistency [Distributed databases]. MPC signing analogous to distributed transaction (multi-party agreement on signature output, rollback on failure). Adaptation: State machine tracking signing progress (initiated → round 1 → ... → completed/aborted), distributed locking preventing concurrent sessions, checkpoint/rollback for recovery. No MPC wallet providers publish session management specifications [Market observation, 2025]. Split-brain prevention: Distributed systems use quorum (majority vote) + fencing (prevent partitioned minority from acting) to avoid split-brain [Distributed systems theory]. MPC adaptation: Verify ≥t parties reachable before starting signature (quorum check), abort if partition detected mid-signing (fencing), distributed consensus on session state. Implementation complexity: Requires additional communication rounds (quorum verification, partition detection heartbeats), adds latency overhead 100-500ms [Estimated]. No comprehensive solution: Industry lacks standard MPC fault tolerance framework (protocols, timeout configurations, recovery procedures unspecified); each provider implements ad-hoc solutions (unpublished, inconsistent across providers).
   
   - **Known facts, assumptions, and uncertainties**: 
     - **Facts**: Byzantine Fault Tolerance requires ≥(2t+1) parties for t Byzantine faults, Crash Fault Tolerance requires ≥(t+1) parties for t crash faults [NIST: BFT Consensus, 2023]; CAP theorem: network partition forces choice between consistency and availability [Distributed systems theory]; MPC threshold signatures (GG20, CGGMP21) require 3-5 protocol rounds with network communication per round [Paper: CGGMP21, 2021]; Common MPC wallet configurations: 2-of-2 (zero fault tolerance), 2-of-3 (1 party failure tolerated), 3-of-5 (2 party failures tolerated) [Market observation, 2025]; Cloud provider SLAs: 99.9% availability implies 8.7h/year downtime (AWS, GCP, Azure regional failures occur [Cloud SLA data]); Manual recovery current standard: 2-6h average operator response time [Estimated from incident reports]; Zero public MPC wallet providers publish network partition handling specifications [Market observation, 2025].
     - **Assumptions**: Estimated 5-10% of institutional users experience partition-related delays monthly (based on support ticket volume from anecdotal provider discussions, not public data); Average 2-6h manual recovery time (estimated from incident reports, no systematic study); Institutional trading opportunity cost $50K-$500K per partition incident (estimated from high-frequency trading volume + market volatility, not quantified); Per-provider implementation cost $2M-$10M (distributed systems research $500K-$2M, protocol enhancement $1M-$4M, testing $500K-$2M, monitoring $500K-$1M, documentation $200K-$500K, assumes large provider scale); Automated recovery can reduce downtime to <5min for 90% of transient partitions (assumes protocol enhancements + monitoring infrastructure successful); Dynamic timeouts can reduce false-positive aborts by 80% (assumes adaptive algorithms based on network latency monitoring effective); Asynchronous MPC 5-10× slower than semi-synchronous (research estimates, not production benchmarks); Heartbeat overhead for partition detection <1% network traffic increase (1s heartbeat frequency, assumes lightweight messages).
     - **Uncertainties**: What percentage of partition incidents are transient (<5min) vs. prolonged (>1h)? (Determines feasibility of automated recovery vs. manual intervention always required); Can asynchronous MPC achieve acceptable performance for production? (Theory demonstrates feasibility but practical implementations limited, overhead quantification needed); What is optimal timeout configuration? (Trade-off between false-positive aborts and delayed failure detection, depends on network latency distribution, institutional vs. retail user tolerance); How frequently do split-brain scenarios occur in practice? (Zero public incidents reported, but risk unquantified; proactive testing via chaos engineering needed); Can distributed consensus overhead be minimized? (Quorum verification, partition detection add latency; optimization possible but unclear how much); Will users accept occasional 5min recovery delays? (Institutional users likely tolerate if communicated transparently, retail users may churn; user tolerance data lacking); How to balance fault tolerance and security? (Reducing threshold t increases availability but weakens security; institutional users' risk appetite unclear); Should MPC wallets prioritize availability over consistency in some scenarios? (CAP theorem trade-off; perhaps time-insensitive transactions tolerate eventual consistency?); What SLA tier appropriate for digital asset custody? (99% vs. 99.9% vs. 99.99%; regulatory guidance absent); Can multi-cloud deployments eliminate all partition risk? (Cross-cloud latency, correlated failures remain; diminishing returns vs. complexity).

---

## Glossary

- **Asynchronous MPC**: MPC protocols tolerating arbitrary message delays without timeouts; theoretically robust against network partitions but 5-10× slower than semi-synchronous MPC; limited production implementations.
- **Byzantine Fault Tolerance (BFT)**: System's ability to tolerate malicious adversaries presenting different behaviors to different parties; requires ≥(2t+1) total parties to tolerate t Byzantine faults.
- **CAP Theorem**: Distributed systems under network partition must choose between Consistency (all nodes see same state) or Availability (system continues operating); MPC wallets prioritize consistency (no double-signing) over availability.
- **Chaos Engineering**: Practice of systematically injecting faults (network partitions, latency, packet loss) to validate system robustness; pioneered by Netflix, applicable to MPC wallet testing.
- **Crash Fault Tolerance (CFT)**: System's ability to tolerate node crashes (unreachable, not malicious); requires ≥(t+1) total parties to tolerate t crash faults; network partitions behave as crash faults.
- **Heartbeat**: Periodic message exchanged between distributed parties to detect failures; 1s frequency enables <10s partition detection but increases network overhead; trade-off between detection latency and bandwidth.
- **Liveness**: Distributed system property ensuring progress despite failures; MPC wallet liveness means transactions eventually complete (or fail explicitly) rather than hang indefinitely.
- **Quorum**: Minimum number of parties required for valid operation; MPC threshold signature requires ≥t parties (quorum); split-brain prevention requires verifying quorum before starting signature.
- **Recovery Time Objective (RTO)**: Target time to restore service after failure; MPC wallet RTO <5min for automated recovery vs. 2-6h manual intervention currently.
- **Semi-synchronous Network**: Network model assuming bounded message delays (messages delivered within timeout period); MPC protocols typically assume semi-synchronous, timeout configurations critical.
- **Session Management**: State machine tracking multi-party protocol progress (initiated → round 1 → ... → completed/aborted); ensures state consistency, prevents split-brain, enables recovery after partition.
- **Split-brain**: Scenario where partitioned subsystems independently believe themselves quorum, risk generating conflicting outputs (e.g., double-signing); prevented via quorum enforcement and distributed consensus.

---

## Reference

### Distributed Systems Fault Tolerance

- [NIST: BFT Consensus, 2023] - "State Machine Replication and Consensus with Byzantine Adversaries" - NIST IR 8460 - Byzantine Fault Tolerance requiring ≥(2t+1) parties for t Byzantine faults, crash fault tolerance requiring ≥(t+1) for t crash faults (https://nvlpubs.nist.gov/nistpubs/ir/2023/NIST.IR.8460.ipd.pdf)
- [Distributed Systems Theory] - CAP Theorem (Eric Brewer, 2000) - Network partition forces choice between consistency and availability
- [Distributed Systems Theory] - Split-brain prevention via quorum + fencing in distributed databases and cluster management

### MPC Threshold Signature Protocols

- [Paper: CGGMP21, 2021] - "UC Non-Interactive, Proactive, Threshold ECDSA with Identifiable Aborts" - CGGMP21 threshold ECDSA protocol with 3-5 rounds (https://eprint.iacr.org/2021/060)
- [Paper: GG20] - "One Round Threshold ECDSA with Identifiable Abort" - GG20 protocol (predecessor to CGGMP21)
- [Research: Asynchronous MPC] - Active research area exploring MPC without synchrony assumptions, 5-10× performance overhead estimated

### Cloud Provider SLAs

- [Cloud SLA Data] - AWS, GCP, Azure publish 99.9% regional availability SLAs (8.7 hours/year expected downtime), 99.99% for multi-region deployments (52 minutes/year)

### Industry Practice

- [Industry Practice] - Chaos engineering (Netflix Chaos Monkey, Amazon Game Days) demonstrating value of proactive fault injection testing for distributed systems
- [Anecdotal] - Network partition incidents from cloud outages, cross-cloud latency, firewall misconfigurations reported in MPC wallet provider support channels (not systematically documented)

### Market Observation

- [Market observation, 2025] - Manual scan of major MPC wallet providers (Fireblocks, Coinbase WaaS, ZenGo, BitGo, Web3Auth, Privy, Magic, Turnkey, Fordefi) - Zero publicly document network partition handling specifications, timeout configurations, recovery procedures, or availability SLAs as of November 2025

### Incident Reports

- [Estimated from incident reports] - 2-6h average manual recovery time for network partition incidents (anecdotal from provider support discussions, no systematic public study)
- [Estimated] - 5-10% of institutional MPC wallet users experience partition-related transaction delays monthly (inferred from support ticket volume discussions, not verified public data)

### Institutional Impact

- [Estimated] - $50K-$500K opportunity cost per partition incident for institutional high-frequency trading operations (estimated from trading volume, market volatility, downtime duration; not systematically quantified)
