1. Q: Your MPC wallet startup has 3 engineers and 8 weeks to launch an MVP supporting Ethereum. A senior engineer proposes implementing hexagonal architecture from day one. What would you do and why?
   A: **Decision: Implement simplified layered architecture first, refactor to hexagonal post-MVP**. **Rationale**: (1) **Time constraint**—hexagonal adds 20-30% development time (2-3 extra weeks), missing launch window. (2) **Team size**—3 engineers lack bandwidth for abstraction layers; need all hands on features. (3) **Uncertainty**—don't know which protocols/storage will be needed long-term; premature abstraction is waste. (4) **MVP validation**—need to prove product-market fit before architectural investment. **Actions**: (a) Build monolithic Ethereum-only wallet with clear module boundaries (signing, storage, API) but direct dependencies. (b) Document areas needing abstraction in tech debt backlog. (c) Allocate 2 weeks post-launch for hexagonal refactor when team grows to 5+. (d) Accept 10-15% code duplication now for 80% faster time-to-market. **Trade-offs**: Short-term velocity (+80% faster) vs. long-term maintenance (+40% harder to add chains). For early-stage startup, survival > perfect architecture.

1. Q: Your MPC wallet serves 50K users. Monitoring shows GG20 signing has 15% failure rate with 300ms p95 latency. CTO wants to adopt FROST to hit 120ms / <2% failure target. What would you do and why?
   A: **Decision: Investigate root cause of 15% failures before protocol switch; FROST may not solve underlying issue**. **Actions**: (1) **Analyze failure modes**—are failures from timeouts (network), participant unavailability (offline devices), or protocol errors? FROST only helps if issue is round complexity. (2) **Measure network RTT**—if p95 RTT is 250ms, then 300ms total is already optimal (protocol is 50ms). FROST saves 40ms protocol time but won't fix 250ms network. (3) **Test with adaptive timeouts first**—increase timeout to 500ms dynamically, measure if failure rate drops to <5%. If yes, network is issue, not protocol. (4) **If network isn't issue**, pilot FROST on 5% of traffic (feature flag) for 2 weeks, compare metrics: success rate, latency, operational complexity. **If pilot shows improvement**, gradual rollout. **If network is issue**, implement geographic routing (match users to nearby participants), expect 50% latency reduction at 10% infrastructure cost. **Cost-benefit**: FROST migration = 6-8 engineer-weeks. Network optimization = 2-3 weeks + $5K/month. Choose based on root cause.

1. Q: Your institutional MPC wallet handles $100M in custody. Security audit recommends moving from 3-of-5 threshold to 5-of-7, citing "inadequate Byzantine tolerance." This increases signing latency from 150ms to 280ms. What would you do and why?
   A: **Decision: Hybrid approach—5-of-7 for transactions >$100K, keep 3-of-5 for <$100K**. **Rationale**: (1) **Risk-proportional security**—$1M transfer warrants 280ms latency; $500 transfer doesn't. (2) **User segmentation**—high-value institutional users accept latency for security; retail users churn at >200ms. (3) **Quantified risk**—3-of-5 means attacker needs 3 compromises (60% of keys); 5-of-7 means 5 compromises (71%). For $100M pool, ~$400K annual insurance cost difference vs. $2M potential user churn from poor UX. **Actions**: (a) Implement threshold policy engine: `if (txn.value > $100K) use_threshold(5, 7) else use_threshold(3, 5)`. (b) Add UI friction for high-value transactions (deliberate slow-down, multi-factor confirmation). (c) Provide institutional accounts option to enforce 5-of-7 globally (some compliance teams require it). (d) Monitor conversion rates pre/post change, A/B test threshold boundaries ($50K, $100K, $250K). **Expected outcome**: <5% impact on retail UX, 70% institutional approval rate improvement, $150K annual insurance savings.

1. Q: Your MPC wallet uses PostgreSQL for key shard storage with AES-256 encryption. A junior engineer proposes migrating to cloud HSM (AWS CloudHSM) citing security best practices, but cost would increase from $500/month to $8K/month. What would you do and why?
   A: **Decision: Implement envelope encryption with cloud KMS ($100/month) as intermediate step before full HSM migration**. **Rationale**: (1) **Incremental security**—current hardcoded encryption keys are main vulnerability; KMS solves 80% of risk at 20% of HSM cost. (2) **Cost-benefit**—$8K/month = $96K/year for 10K-user product. Break-even requires $9.60 annual revenue per user just to cover HSM. (3) **Compliance**—most regulations (PCI DSS Level 2-3) accept KMS; Level 1 (>6M transactions/year) requires HSM. (4) **Scale phasing**—migrate to HSM when crossing $10M AUM or 50K users (justify cost with revenue). **Actions**: (a) Phase 1 (Month 1-2): Migrate encryption keys to AWS KMS, implement envelope encryption. Cost: $100/month, security improvement: 80%. (b) Monitor KMS API latency (adds 5-10ms per operation), ensure <200ms total signing latency maintained. (c) Phase 2 (Month 12+): When AUM > $10M, migrate to CloudHSM for subset of institutional accounts (selective deployment). Cost impact: 50% of keys on HSM ($4K/month), 50% on KMS. (d) Communicate roadmap to customers: "KMS now, HSM within 12 months for premium accounts." **Trade-off**: Delayed HSM vs. $90K saved in year 1.

1. Q: Your MPC wallet signing fails show a pattern: 80% happen during a 2-hour window (7-9pm UTC) when transaction volume spikes 10x. Current architecture has no autoscaling. What would you do and why?
   A: **Decision: Implement presignature pool + horizontal scaling + request queuing**. **Rationale**: (1) **Root cause**—spike overwhelms fixed capacity, causing timeouts. (2) **Presignature pool** precomputes signatures during off-peak (solves 60-70% of latency), (3) **Horizontal scaling** adds capacity during peak (solves throughput), (4) **Queuing** manages burst gracefully. **Actions**: (a) Deploy presignature pool service (50-100MB RAM per instance) to run continuously, targeting 1000 presignatures ready at peak time. Expected latency reduction: 200ms → 80ms. (b) Implement Kubernetes HPA (Horizontal Pod Autoscaler) with CPU and queue depth metrics: scale from 3 pods (baseline) to 15 pods (peak) between 6:30-9:30pm. Cost: $200/day during peak vs. $50/day baseline—acceptable for failure prevention. (c) Add Redis-backed job queue (Bull/Sidekiq) with max queue size 10K, priority ordering (high-value transactions first). Reject requests beyond 10K queue depth with 503 + retry-after header. (d) Set SLO: p95 latency <500ms during peak (realistic for 10x load), <150ms off-peak. **Expected results**: Failure rate drops from 20% to <3% during peak, autoscaling cost ~$4K/month fully amortized by prevented customer churn (retention improvement >1%).

1. Q: Your wallet uses synchronous CQRS where writes block until read models are updated. Users complain about slow transaction confirmations (200ms avg). A developer proposes switching to async CQRS with 20-40ms eventual consistency lag. What would you do and why?
   A: **Decision: Implement hybrid CQRS—async for most operations, sync for read-your-writes on critical paths**. **Rationale**: (1) **User mental model**—users expect to see balance update immediately after transaction; 40ms lag feels broken. (2) **Not all reads are equal**—transaction history can be stale; available balance cannot. (3) **Performance gains**—async CQRS achieves 10x read throughput, but sync CQRS on critical path is acceptable for 5-10% of operations. **Actions**: (a) Categorize operations: **Sync required** (post-transaction balance query, send immediately after receive, transfer after balance check). **Async acceptable** (transaction history pagination, analytics, notifications). Estimate: 10% sync, 90% async. (b) Implement dual paths: Command handler emits event, publishes to Kafka. Read models consume async. For sync operations, command handler also triggers immediate projection update in transaction, returns once complete. (c) Add client-side optimistic updates: When user sends transaction, UI immediately shows expected balance (grayed out), confirms when event processed. Feels instant even with async backend. (d) Monitor consistency lag metrics (p50, p95, p99), set SLO: <40ms lag for 99% of updates. **Expected outcome**: Write latency drops from 200ms to 50ms (75% improvement), read throughput increases 8-10x, user satisfaction on "responsiveness" improves from 60% to 85% (NPS survey).

1. Q: Your MPC wallet is planning multi-chain expansion from Ethereum to Ethereum + Bitcoin + Solana. Current monolithic codebase has Ethereum logic throughout. Team suggests a 4-month rewrite to hexagonal architecture. CTO wants faster launch. What would you do and why?
   A: **Decision: Strangler fig pattern—build new chains with hexagonal adapters, gradually migrate Ethereum**. **Rationale**: (1) **Risk mitigation**—4-month rewrite risks breaking existing production Ethereum wallet serving 100K users. (2) **Revenue timeline**—waiting 4 months means 4 months of lost multi-chain revenue. (3) **Learning**—building 2 new chains informs better abstractions than speculative refactor. **Actions**: (a) Month 1-2: Create `ChainAdapter` interface based on Ethereum needs (`buildTransaction`, `signTransaction`, `broadcast`, `getBalance`). Build Bitcoin and Solana adapters implementing this interface. Core signing logic stays monolithic but routes through new adapters for BTC/SOL. (b) Month 2-3: Launch BTC/SOL in beta (10% of users), validate adapter pattern works, refine interface based on real usage (likely 2-3 iterations). (c) Month 4-6: Build `EthereumAdapter` implementing same interface. Route 10% of Ethereum traffic through adapter (feature flag), monitor for regressions. Gradually increase to 100%. (d) Month 6: Deprecate monolithic Ethereum code once adapter at 100% traffic. **Benefits**: New chains launched in 2 months instead of waiting 4; lower risk (BTC/SOL isolated); learned abstractions from real usage, not speculation. **Trade-offs**: 20% code duplication temporarily (both old monolithic Ethereum and new adapter exist), but eliminated once migration complete.

1. Q: Your MPC wallet circuit breaker opens after 3 consecutive failures, remaining open until manual operator reset. During a brief 30-second network partition at 3am, the circuit opened and stayed open for 6 hours until morning ops team noticed. What would you do and why?
   A: **Decision: Implement automatic half-open state with exponential backoff testing**. **Rationale**: (1) **Availability requirement**—99.9% SLA allows 43 minutes downtime/month; 6-hour outage from transient failure is unacceptable. (2) **Transient failure recovery**—most failures (network blips, participant restarts) are temporary; circuit should auto-recover. (3) **Ops burden**—on-call engineers burnt out from 3am pages for self-healing issues. **Actions**: (a) Add state machine: After circuit opens, wait `cooldown_period` (start at 30 seconds), then transition to half-open. In half-open, allow single test request. If succeeds, close circuit. If fails, reopen, double cooldown (exponential backoff: 30s → 60s → 120s → 240s, max 10 minutes). (b) Implement synthetic health checks every 15 seconds (lightweight ping to each participant). Health check success doesn't count toward user quota but can trigger half-open transition. (c) Add manual override: `/admin/circuit/{participant_id}/force_close` endpoint for emergency operator intervention (kept for edge cases). (d) Monitoring: Alert if circuit in half-open state >5 minutes (indicates persistent issue needing investigation), separate from transient opens. **Expected results**: MTTR drops from 6 hours (worst case) to 30 seconds (transient failures), 99.95% availability vs. previous 99.5%, on-call pages drop by 70%, operator manual actions drop from 10/week to 1/week.

1. Q: Your MPC wallet stores transaction events in event store for audit compliance (7-year retention). Event store is now 5TB, costing $10K/month in database storage and slowing queries. What would you do and why?
   A: **Decision: Implement hot/warm/cold storage tiering with event archival**. **Rationale**: (1) **Access pattern**—99% of queries are for events <90 days old; older events only accessed during audits (rare). (2) **Cost optimization**—S3 cold storage is $1-2/TB/month vs. $200/TB/month for database. 5TB in database = $1K/month, 4.5TB (events >90 days) to S3 = $10/month—$990/month savings. (3) **Compliance**—7-year retention doesn't require 7 years in hot database; immutable S3 with Glacier acceptable. **Actions**: (a) **Hot tier** (database): Keep events <30 days in primary database for fast queries (<10ms). Size: ~500GB, cost: $100/month. (b) **Warm tier** (compressed database or S3 Standard): Move events 30-90 days old to warm storage. Queries take 50-100ms but acceptable for less frequent access. Size: ~1TB, cost: $50/month (S3 Standard). (c) **Cold tier** (S3 Glacier): Archive events >90 days to Glacier. Retrieval takes 3-5 hours but acceptable for audit requests (1-2 times/year). Size: 3.5TB and growing, cost: $3.50/month. (d) Implement lifecycle policies: Automatic migration at 30-day and 90-day boundaries. (e) Build admin tool for audit event retrieval: Submit request, get email when Glacier retrieval completes (3-5 hours), download via signed S3 URL. **Expected outcome**: Storage cost drops from $10K/month to $153.50/month (98.5% reduction), query performance for active data improves 3-5x (smaller hot dataset), compliance maintained (all events immutable with retrieval process).

1. Q: Your MPC wallet team has 8 engineers. Key generation DKG ceremony codebase is 15K LOC of complex Rust with no tests (written quickly for launch). A new regulation requires full audit trail of DKG operations. What would you do and why?
   A: **Decision: Freeze feature development for 4 weeks; allocate 4 engineers to refactor DKG with test coverage and event sourcing for audit trail**. **Rationale**: (1) **Regulatory risk**—non-compliance could result in $100K-$1M fines or forced shutdown (existential). (2) **Technical debt**—15K LOC untested code is ticking time bomb; bug in DKG could compromise all keys. (3) **Opportunity cost**—4 weeks of 4 engineers = ~$120K salary cost, but potential fine is 10x higher. (4) **Long-term velocity**—untested code slows all future DKG changes; investment pays back in 6 months. **Actions**: (a) Week 1-2: **Characterization tests**—write integration tests that capture current behavior (even if buggy), prevent regressions during refactor. Target: 60% path coverage of critical flows (key generation, signing, recovery). (b) Week 2-3: **Event sourcing refactor**—emit events for every DKG step: `DKGInitiated`, `CommitmentReceived`, `ShareGenerated`, `DKGCompleted`. Store in immutable event log. Build audit query tool: "Show all DKG events for wallet X between dates Y-Z." (c) Week 3-4: **Unit test coverage**—extract pure functions from monolithic code, write unit tests achieving 80% line coverage for cryptographic operations. (d) Week 4: **Load testing & documentation**—run DKG under stress (100 concurrent ceremonies), document failure modes, create runbook for ops team. **Expected results**: Audit trail enables compliance (regulation met), test coverage eliminates 70% of future production bugs, team velocity improves 30% in months 2-6 (easier to modify tested code), refactor cost: $120K salary + 4 weeks feature delay vs. risk mitigation: >$1M potential fine avoided.

1. Q: Your MPC wallet's signing latency p95 is 180ms (target: <100ms). Profiling shows 60ms in network communication (3 round trips), 40ms in cryptographic computation, 80ms in key shard retrieval from HSM. What would you do and why?
   A: **Decision: Multi-pronged optimization—(1) presignature pool eliminates 2 round trips, (2) HSM connection pooling, (3) batch shard requests**. **Rationale**: (1) **Network latency** (60ms) is largest addressable component—presignatures reduce rounds from 3 to 1. (2) **HSM latency** (80ms) likely from connection overhead—pooling + batching. (3) **Crypto computation** (40ms) is irreducible without specialized hardware—not priority. **Actions**: (a) **Presignature pool** (eliminates 40ms network): Pre-compute signatures during idle time, store in Redis. When user signs, fetch presignature + complete in 1 round instead of 3. Savings: 60ms → 20ms network. Cost: 50-100MB RAM per instance, continuous CPU usage. (b) **HSM connection pooling** (reduces HSM overhead from 80ms to 30ms): Maintain 10 persistent connections to HSM, reuse instead of connect/disconnect per request. Connection overhead: 50ms → 0ms. HSM query time: 30ms (irreducible). (c) **Batch shard requests** (if multiple signatures simultaneous): Request 10 shards in single HSM call instead of 10 separate calls. Amortizes connection overhead across requests. Effective per-request: 30ms → 10ms (under load). (d) **Protocol optimization** (if possible): If using GG20, evaluate FROST (reduces rounds). But presignatures already achieve similar benefit. **Expected results**: Network: 60ms → 20ms (-40ms), HSM: 80ms → 30ms (pooling) or 10ms (batching) (-50-70ms), Total: 180ms → 90ms baseline, 70ms under load. Meets <100ms target. **Investment**: 2 engineer-weeks implementation + $200/month infrastructure (presignature instances) vs. 50% latency improvement.
