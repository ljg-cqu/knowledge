# Two-Tier Problem Classification System

**Last Updated**: 2025-11-29  
**Status**: Final v1.0  
**Owner**: Documentation Team

## Overview

**üìã Navigation**: See [Table of Contents](#table-of-contents) for priority indicators (Critical/Important/Optional).

### Key Concepts

**MECE** (Mutually Exclusive, Collectively Exhaustive): Categories don't overlap and cover all possibilities.  
**Pareto Principle (80/20)**: ~80% of effects come from ~20% of causes; used for quantitative prioritization.

### System Purpose

Two-tier classification for multi-dimensional problems:
- **Tier 1**: Single root cause (near-MECE)
- **Tier 2**: Multiple impact dimensions (multi-tag)
- **Pareto Scoring**: 1-300 scale prioritization

**Core Insight**: Problems have ONE root cause but MULTIPLE impacts.

**Example**: Database outage ‚Üí Root: Technical/Architectural + Impacts: Performance, UX, Economic, Reputation

### Context

**Problem**: Organizations struggle to prioritize problems across technical, regulatory, economic, and operational dimensions. Single-tier systems lose multi-dimensional context.

**Audience**: Product managers, engineering leaders, operations directors, executives.

**Prerequisites**: ‚â•6mo historical data (incidents, costs, frequency), cross-functional participation, tracking infrastructure.

**Timeline**: Mo 1-3 (pilot, 70% agreement) ‚Üí Mo 4-6 (rollout, 85%) ‚Üí Mo 7-12 (optimization, 90%) ‚Üí Year 2+ (20-30% improvement).

**Resources**: 2-4 hrs/problem initially (‚Üí30-60 min); 3-person team; quarterly updates (1-2 days); annual review (1 week).

**Constraints**: ‚â•20 problems; stable problems; finite resources; Pareto may not apply‚Äîvalidate with data.

**Quick Start**: [CRITICAL] sections (15-20 min) ‚Üí [Important] sections (+30-45 min) ‚Üí [Optional] as needed.

### ‚ö†Ô∏è Data Quality Notice

Quantitative examples (dollar amounts, percentages) are **illustrative estimates** flagged with **[^footnote]** markers. Before critical decisions: verify claims, replace with organization-specific data, cite primary sources, flag estimates explicitly. See [Data Quality and Citations](#data-quality-and-citations).

---

## Table of Contents

**[CRITICAL]** = Essential (15-20 min) | **[Important]** = Recommended (30-45 min) | **[Optional]** = Reference

1. **[Overview](#overview)** ‚Äî **[CRITICAL]**
   - [Key Concepts](#key-concepts) | [System Purpose](#system-purpose) | [Context](#context)
2. **[Tier 1: Primary Root Cause Categories](#tier-1-primary-root-cause-categories-mece-optimized)** ‚Äî **[CRITICAL]**
   - External Threat | Technical Constraint | Regulatory Mandate | Design/Standards Gap | Human/Organizational | Economic/Market
3. **[Tier 2: Impact Dimensions](#tier-2-impact-dimensions-multi-tag-system)** ‚Äî **[Important]**
   - Security | Regulatory | Operational | UX | Economic | Performance | Trust/Reputation
4. **[Problem Severity](#problem-severity-classification)** ‚Äî **[CRITICAL]**
5. **[80/20 Pareto Framework](#8020-pareto-prioritization-framework)** ‚Äî **[Important]**
   - Impact Magnitude (1-10) | Frequency (1-10) | Criticality Weight (1-3√ó) | Resource Allocation Tiers (S/A/B/C)
6. **[Usage Guidelines](#usage-guidelines)** ‚Äî **[CRITICAL]**
   - Decision Tree | Step-by-Step Process
7. **[Problem Metadata Template](#problem-metadata-template)** ‚Äî **[Important]**
8. **[Validation Against MECE](#validation-against-mece-principles)** ‚Äî **[Optional]**
9. **[Applicability and Limitations](#applicability-and-limitations)** ‚Äî **[Important]**
10. **[Comparison with Alternatives](#comparison-with-alternative-approaches)** ‚Äî **[Optional]**
11. **[Success Criteria](#success-criteria-and-measurement)** ‚Äî **[Important]**
12. **[Maintenance](#maintenance-and-evolution)** ‚Äî **[Optional]**
13. **[Data Quality](#data-quality-and-citations)** ‚Äî **[Important]**
14. **[QA Checklist](#quality-assurance-checklist)** ‚Äî **[CRITICAL]**

---

## Tier 1: Primary Root Cause Categories (MECE-Optimized)

**Principle**: Identify the **primary driver**‚Äîwhat fundamentally creates this problem? If removed, the problem would be eliminated.

### 1. External Threat (Adversarial/Environmental)

**Definition**: Adversaries, natural disasters, or external disruptions beyond organizational control.

**Sub-categories**: Malicious Actors | Social Engineering | Insider Threats | Natural/Geopolitical

**Examples**: Ransomware, supply chain disruptions, payment fraud ($485B annual [^1]), DDoS

**Traits**: Ceases if adversaries/shocks absent; requires defense/resilience; mitigation only

---

### 2. Technical/Architectural Constraint

**Definition**: Fundamental technical, physical, or mathematical limitations requiring paradigm shift.

**Sub-categories**: Performance Limits | Scalability Boundaries | Compatibility | Physical/Mathematical Limits

**Examples**: O(n¬≤) complexity, MRI scan time (physics), CNC precision (vibration), last-mile delivery

**Traits**: Not "fixable" by process; requires technological breakthrough; hard constraints

---

### 3. Regulatory/Legal Mandate

**Definition**: Compliance, licensing, or jurisdictional requirements with non-compliance consequences.

**Sub-categories**: Compliance | Licensing & Classification | Tax & Financial | Privacy & Data Protection | Jurisdictional Fragmentation

**Examples**: HIPAA ($1.5M implementation [^2]), KYC/AML (15-30% abandonment, $5.1B fines [^3]), GDPR deletion

**Traits**: Disappears if regulation changes; legal-driven; non-negotiable

---

### 4. Design/Standards Gap (Ecosystem Immaturity)

**Definition**: Lack of standards, protocol fragmentation, or immature tooling/practices.

**Sub-categories**: Protocol Fragmentation | Integration Complexity | Tooling Gaps | Knowledge/Documentation Deficits

**Examples**: Microservices observability (100+ tools, $300K-$1M [^4]), EHR interoperability (FHIR <40% [^5]), IoT (50+ protocols)

**Traits**: Solvable via standardization; improves with ecosystem maturity; multi-stakeholder collaboration

---

### 5. Human/Organizational Factor

**Definition**: Coordination failures, behavioral errors, organizational conflicts, or governance disputes.

**Sub-categories**: Multi-Party Coordination | Human Error | Organizational Conflict | Knowledge/Training Gaps

**Examples**: Deployment errors (70% outages, $100K-$500K [^6]), medical errors (250K deaths/yr US [^7]), approval bottlenecks (6-18mo)

**Traits**: Technology alone insufficient; requires governance/training; high variance

---

### 6. Economic/Market Constraint

**Definition**: Cost-viability tradeoffs, market structure gaps, or economic accessibility barriers.

**Sub-categories**: Cost-Benefit Misalignment | Market Structure Gaps | Economic Accessibility | Operational Economics

**Examples**: MRI unaffordable (rural hospitals: $500K vs $100K budget), SMB security costs, tutoring ($50-100/hr), automation ROI

**Traits**: Technically feasible but economically prohibitive; improves with scale/maturity; may need new models

[^1]: Global aggregate estimate; verification needed  
[^2]: Industry estimate; verification needed  
[^3]: Survey-based; verification needed  
[^4]: Integration cost from surveys; high variance  
[^5]: Industry estimate; verification needed  
[^6]: Postmortem data; cost varies by size  
[^7]: Widely cited; methodology debated; verification needed

---

## Tier 2: Impact Dimensions (Multi-Tag System)

**Purpose**: Tag ALL applicable dimensions (typically 3-5/problem).

### üîí Security & Risk
**Scope**: Vulnerability to loss, breach, safety incidents, catastrophic failure  
**Indicators**: Dollar losses, incident frequency, breach rates

### üìã Regulatory & Legal Compliance
**Scope**: Legal risk, compliance burden, non-compliance penalties  
**Indicators**: Fines, compliance costs, audit requirements

### ‚öôÔ∏è Operational Efficiency
**Scope**: Process overhead, coordination burden, system reliability  
**Indicators**: Process time, staff hours, error rates, downtime

### üë• User Experience & Adoption
**Scope**: Usability barriers, abandonment, satisfaction deficits  
**Indicators**: Completion rates, abandonment %, support tickets, NPS

### üí∞ Economic & Cost
**Scope**: Financial burden, cost-benefit viability, pricing accessibility  
**Indicators**: Implementation costs, operational expenses, unit economics, ROI

### ‚ö° Technical Performance
**Scope**: Speed, throughput, latency, scalability limitations  
**Indicators**: Response time, throughput (req/sec), processing delays

### ü§ù Trust & Reputation
**Scope**: Market confidence, brand credibility, stakeholder trust  
**Indicators**: NPS, customer churn, brand sentiment, incident frequency

---

## Problem Severity Classification

**[CRITICAL]**: Existential threats, catastrophic failures, regulatory violations, irreversible harm (business-ending, >$10M losses, regulatory action, fatalities)

**[Important]**: Significant barriers, competitive disadvantages, operational inefficiencies (market position erosion, customer attrition, >$1M annual cost)

**[Moderate]**: Incremental improvements, optimization opportunities, edge cases (efficiency gains, satisfaction improvements, <$1M annual value)

---

## 80/20 Pareto Prioritization Framework

**Purpose**: Data-driven prioritization using three-axis scoring to identify "vital few" (20% causing 80% impact).

**Formula**: `Priority Score = Impact (1-10) √ó Frequency (1-10) √ó Criticality (1.0-3.0)` ‚Üí Range: 1-300

### Axis 1: Impact Magnitude

| Score | Range | Examples |
|-------|-------|----------|
| **10** | Catastrophic ($1B+, 10M+ stakeholders) | Major outages, system failures |
| **8** | Severe ($100M-$1B, 1M-10M) | Fraud losses, institutional burdens |
| **6** | Significant ($10M-$100M, 100K-1M) | Compliance costs, major integrations |
| **4** | Moderate ($1M-$10M, 10K-100K) | Technology implementations |
| **2** | Minor ($100K-$1M, 1K-10K) | Process improvements, tooling |
| **1** | Minimal (<$100K, <1K) | Small optimizations |

**Sources**: Incident logs, financial systems

### Axis 2: Frequency

| Score | Frequency | Examples |
|-------|-----------|----------|
| **10** | Constant (daily) | Production incidents, fraud |
| **8** | Very Frequent (weekly) | Approvals, deployments |
| **6** | Frequent (monthly) | Compliance, maintenance |
| **4** | Periodic (quarterly/annual) | Audits, renewals |
| **2** | Occasional (multi-year) | Governance disputes |
| **1** | Rare (5+ years) | Black swans |

**Sources**: Incident logs, operational cycles  
**Aggregation**: 1,000 orgs √ó "rare" (2) ‚Üí ecosystem "frequent" (6)

### Axis 3: Criticality Weight

| Severity | Weight | Rationale |
|----------|--------|-----------|
| **[CRITICAL]** | **3.0√ó** | Existential threats, irreversible harm, life-safety |
| **[Important]** | **2.0√ó** | Competitive differentiation, market barriers |
| **[Moderate]** | **1.0√ó** | Incremental improvements, optimizations |

### Example Calculations

| Problem | Impact | Freq | Crit | Score | Tier |
|---------|--------|------|------|-------|------|
| **Security Breaches** | 8 | 10 | 3.0 | **240** | S (Top 5%) |
| **Medical Diagnosis Delays** | 8 | 6 | 3.0 | **144** | S (Top 10%) |
| **Deployment Errors** | 6 | 10 | 2.0 | **120** | S (Top 10%) |
| **Supply Chain Fragmentation** | 4 | 8 | 2.0 | **64** | A (Top 20%) |
| **Approval Deadlocks** | 8 | 2 | 3.0 | **48** | A (Top 30%) |

### Resource Allocation Tiers

| Tier | Score | % Problems | Budget | Action |
|------|-------|------------|--------|--------|
| **S (Vital Few)** | ‚â•100 | ~20% | **60-70%** | Immediate investment, continuous monitoring |
| **A (Important Many)** | 50-99 | ~30% | **20-30%** | Scheduled improvements, automation |
| **B (Useful Minority)** | 20-49 | ~30% | **10-20%** | Incremental optimizations, defer unless strategic |
| **C (Trivial Many)** | <20 | ~20% | **<10%** | Monitor only, opportunistic fixes |

**ROI**: Tier S = ~75-80% impact; 60-70% allocation ‚Üí 3-4√ó concentration vs. uniform; expect 30-50% improvement Year 1

---

## Usage Guidelines

### Classification Decision Tree

```
What fundamentally creates this problem?
‚îú‚îÄ Adversaries/external forces? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ [1] External Threat
‚îú‚îÄ Technical/physical limitation? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ [2] Technical/Architectural Constraint
‚îú‚îÄ Required by law/regulation? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ [3] Regulatory/Legal Mandate
‚îú‚îÄ Ecosystem lacks standards/tooling? ‚îÄ‚ñ∫ [4] Design/Standards Gap
‚îú‚îÄ Human coordination/error? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ [5] Human/Organizational Factor
‚îî‚îÄ Economically unviable? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ [6] Economic/Market Constraint

THEN: Apply Tier 2 impact tags ‚Üí Assign severity ‚Üí Calculate priority score
```

### Step-by-Step Process

**Step 1: Tier 1 Root Cause** ‚Äî "What fundamentally creates this? If eliminated, problem disappears?"  
*Rule*: Choose most upstream cause.  
*Example*: Database slow query (5-10s) ‚Üí O(n¬≤) complexity ‚Üí **2. Technical/Architectural > Performance Limits**

**Step 2: Tier 2 Impact Tags** ‚Äî "Which dimensions affected?" (3-5 tags)  
*Example*: ‚úÖ ‚ö° Performance (5-10s) | üë• UX (15% abandonment) | üí∞ Economic ($600K/yr loss) | ü§ù Reputation (complaints)

**Step 3: Severity** ‚Äî Critical (existential), Important (competitive), or Moderate (incremental)?  
*Example*: Competitive disadvantage + $50K/mo ‚Üí **[Important]**

**Step 4: Priority Score** ‚Äî `Impact (1-10) √ó Frequency (1-10) √ó Criticality (1-3√ó)`  
*Example*: 4 √ó 10 √ó 2.0 = **80** ‚Üí **Tier A** ‚Üí Schedule optimization

**Complete Classification**:
```markdown
- **Tier 1**: 2. Technical/Architectural > Performance Limits
- **Tier 2**: ‚ö° Performance, üë• UX, üí∞ Economic, ü§ù Reputation
- **Severity**: [Important]
- **Score**: 80 ‚Üí Tier A (Impact: 4, Freq: 10, Crit: 2.0√ó)
```

---

## Problem Metadata Template

```markdown
---
**Classification**:
- **Tier 1**: [Category] > [Sub-category]
- **Tier 2**: [Tag 1], [Tag 2], [Tag 3]
- **Severity**: [CRITICAL | Important | Moderate] | **Score**: [1-300] ‚Üí [S|A|B|C] (I√óF√óC)

**Attributes**: [Domain] | [Stakeholders]

**Source**: [Source] | [‚úÖ Verified | ‚ö†Ô∏è Estimated] | Updated: YYYY-MM-DD
---
```

**Example**:
```markdown
---
**Classification**:
- **Tier 1**: 5. Human/Organizational > 5.2 Human Error
- **Tier 2**: üîí Security, ‚öôÔ∏è Operational, üí∞ Economic, ü§ù Reputation
- **Severity**: [CRITICAL] | **Score**: 120 ‚Üí S (6√ó10√ó2.0)

**Attributes**: Software | Engineering, Ops, Exec

**Source**: Postmortem DB (2024) | ‚ö†Ô∏è Estimated | Updated: 2025-11-29
---
```

---

## Validation Against MECE Principles

### Mutually Exclusive (Tier 1)

‚úÖ Ransomware ‚Üí External Threat (NOT Human‚Äîpoor hygiene secondary)  
‚úÖ HIPAA ‚Üí Regulatory (NOT Operational‚Äîprocess is consequence)  
‚úÖ Automation Gaps ‚Üí Design Gap (NOT Operational‚Äîoverhead symptom)  
‚úÖ Approval Deadlocks ‚Üí Human/Org (NOT Technical‚Äîsystems work)  
‚úÖ Query Latency ‚Üí Technical (NOT Performance‚Äîthat's impact)

**Result**: Functional mutual exclusivity via **upstream causes** vs. **downstream impacts**.  
**Ambiguous?** Use **primary driver test**: which elimination most directly solves problem?

### Collectively Exhaustive

‚úÖ AI bias ‚Üí Design Gap OR Human Factor  
‚úÖ Climate impacts ‚Üí External Threat  
‚úÖ Skills shortage ‚Üí Human/Organizational  
‚úÖ Pandemic response ‚Üí Human OR External Threat  
‚úÖ Market monopoly ‚Üí Economic/Market

**Result**: All problems map successfully. Category 4 catches ecosystem immaturity.

---

## Comparison with Alternative Approaches

| Approach | Pros | Cons | Best For |
|----------|------|------|----------|
| **Severity-Only** (Critical/High/Medium/Low) | Simple, fast, universal | No root cause, no solution guidance, single dimension | Incident triage, <20 problems |
| **Domain-Specific** (OWASP, ICD-10) | Deep expertise, standardized, tooling support | Not cross-domain, lacks prioritization, overly granular | Single-domain orgs, compliance-driven |
| **Impact-Only** (Security/Economic/etc.) | Multi-dimensional, stakeholder-aligned, easier consensus | No solution guidance, treats symptoms, no MECE | Stakeholder communication, risk registers |
| **Cause-Only** (Root cause without impacts) | MECE achievable, clear solution guidance, simple | Loses multi-dimensional context, harder prioritization | Engineering-focused, single-stakeholder |
| **Ad-Hoc Scoring** (Subjective judgment) | Flexible, context-specific, low overhead | Inconsistent, biased, not auditable, no learning | Small teams, rapidly changing environments |

**Trade-Off Summary**:

| Approach | MECE | Multi-Dim | Quantitative Priority | Cross-Domain | Solution Guidance | Complexity |
|----------|------|-----------|----------------------|--------------|-------------------|------------|
| **Two-Tier** | ‚úÖ (T1) | ‚úÖ (T2) | ‚úÖ | ‚úÖ | ‚úÖ | High |
| **Severity-Only** | ‚ùå | ‚ùå | ‚ö†Ô∏è | ‚úÖ | ‚ùå | Low |
| **Domain-Specific** | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ùå | ‚úÖ | Medium |
| **Impact-Only** | ‚ùå | ‚úÖ | ‚ö†Ô∏è | ‚úÖ | ‚ùå | Medium |
| **Cause-Only** | ‚úÖ | ‚ùå | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | Low |
| **Ad-Hoc** | ‚ùå | ‚ùå | ‚ö†Ô∏è | ‚úÖ | ‚ùå | Very Low |

**Hybrid Recommendations**:
- **Large Enterprise**: Two-Tier (strategic) + Domain-Specific (operational)
- **Startup/SMB**: Severity-Only (first 6-12mo) ‚Üí Two-Tier (when >20 problems)
- **Incident Response**: Severity-Only (triage) + Two-Tier (retrospective)
- **Regulatory-Driven**: Domain-Specific (mandated) + Two-Tier (strategic gaps)

---

## Applicability and Limitations

### When to Use

**‚úÖ Recommended**: Multi-stakeholder/multi-domain problems | Resource allocation | Consistent classification needs | Root cause analysis | Quantifiable data (‚â•6mo history)

**‚ùå Not Recommended**: Simple/single-dimension problems | Novel/unprecedented problems | Real-time incident response | Immediate action needed | Strategic/philosophical decisions

### Limitations and Risks

| Risk | Mitigation |
|------|------------|
| **Classification Ambiguity** (10-15%) | Primary driver test; document secondary causes; quarterly review |
| **Data Quality (GIGO)** | Require sources; flag estimates; quarterly updates |
| **Quantification Bias** | Supplement with stakeholder voting; qualitative assessment |
| **Domain Calibration** | Calibrate during pilot; adjust by context |
| **Over-Reliance** | Combine with feasibility; consider strategic importance |
| **Gaming** | Require verification; external review (Tier S); audit |

### Assumptions

1. Root causes identifiable
2. Problems relatively stable
3. Impact measurable (proxy exists)
4. Resources finite
5. Pareto applies (validate with data)

### Dependencies

**Data**: Incident tracking, financial reporting, operational metrics  
**Organization**: Cross-functional agreement, maintenance commitment  
**Expertise**: Deep problem understanding

---

## Success Criteria and Measurement

### Primary Metrics

| Metric | Baseline | Year 1 | Year 2 | Measurement |
|--------|----------|--------|--------|-------------|
| **Resolution Rate** | 40-60% | +10-15% | +20-30% | (Resolved / Addressed) √ó 100% |
| **ROI Efficiency** | $2-4/$1 | +30-50% | +50-100% | Impact Reduced / Resources Invested |
| **Agreement** | N/A | Mo 1-3: ‚â•70%<br>Mo 4-6: ‚â•85% | ‚â•90% | (Matching / Total) √ó 100% (3 classifiers, 20 problems) |
| **Predictive Accuracy** | N/A | S: ‚â•75% high<br>C: ‚â§20% high | Same | 6mo outcomes, 20/tier |
| **Time-to-Solution** | 3-12mo | -20-30% | -30-50% | Date(Deployed) - Date(Identified) |

### Secondary Indicators

| Indicator | Target | Warning |
|-----------|--------|---------|
| **Satisfaction** (1-5) | ‚â•4.0 (understanding, transparency)<br>‚â•3.5 (trust, overhead) | <3.0 @6mo ‚Üí overhead high |
| **Coverage** | ‚â•95% in 2 weeks | <80% ‚Üí rules complex |
| **Data Quality** | Yr 1: 60-70%<br>Steady: ‚â•80% | Quarterly audit 50 problems |

### Milestones

| Phase | Timeline | Success | Go/No-Go |
|-------|----------|---------|----------|
| **Pilot** | Mo 1-3 | 20 problems, ‚â•70%, training | <60% ‚Üí Refine |
| **Rollout** | Mo 4-6 | 80% coverage, ‚â•85%, 1st cycle | <3.0 satisfaction ‚Üí Reduce overhead |
| **Optimization** | Mo 7-12 | ‚â•95% coverage, ‚â•90%, 10-15% improvement | <10% ROI ‚Üí Recalibrate |
| **Steady** | Yr 2+ | ‚â•80% quality, 20-30% resolution, 30-50% ROI | Not met ‚Üí Major revision |

### Failure Modes

| Symptom | Action |
|---------|--------|
| **Deadlock** (>10% unclassified @30d) | Refine Tier 1, add sub-category |
| **Gaming** | Mandatory verification, external audit (S) |
| **Inversion** (C > S impact) | Recalibrate scales |
| **Revolt** (bypass) | Simplify to Severity-Only 3mo |
| **No Impact** (ROI <5% @12mo) | Analyze decision influence |

### Continuous Improvement

**Quarterly**: Metrics dashboard | Sample 10 (guide solution?) | Disputes ‚Üí refine tree | Predicted vs. actual (>20% mismatch ‚Üí adjust) | Feedback

**Annually**: Validate MECE compliance | Assess Pareto distribution (20% ‚Üí 80% impact?) | Compare ROI to alternatives | Update targets

## Maintenance and Evolution

**Schedule**: Quarterly (recalc scores, adjust tiers) | Semi-Annual (review Tier 1) | Annual (validate MECE, 6 categories) | Major Incidents (update severity)

**Tracking**: Disagreements ‚Üí refine decision tree | Edge cases ‚Üí new sub-categories | Tier S outcomes ‚Üí validate 80/20

---

## Data Quality and Citations

**‚ö†Ô∏è Notice**: Document contains **illustrative estimates** ([^footnotes]). Before decisions: verify, replace with org data, cite sources, flag estimates.

### Data Quality Tiers

| Tier | Use | Requirements |
|------|-----|--------------|
| **Verified** | Tier S | Internal logs/financials <12mo, traceable, ‚â•2 sources |
| **Substantiated** | Tier A/B | Industry reports, named sources, ranges, flagged "estimated" |
| **Order-of-Magnitude** | Tier C (sparingly) | Explain method, flag "rough", note sensitivity |

**Citation Format**: `Impact: $2.3M | Source: [Report, Dept, Date] | Status: ‚úÖ Verified | Updated: YYYY-MM-DD`

### References

**Classification**: Ishikawa (1990), Reason (2000) BMJ 320:768, Perrow (1999)  
**Pareto**: Koch (2011), Juran (1954) Mgmt Review 43:748  
**MECE**: Minto (2009)  
**Analysis**: Ritchey (2011), Checkland (1999)  
**Risk**: Hubbard (2009), ISO 31000:2018

---

## Quality Assurance Checklist

### Problem Documentation

‚òê **Context**: Problem, stakeholders, constraints, scale  
‚òê **Data**: Impact/frequency verified (‚â•6mo), sources, estimates flagged  
‚òê **Classification**: Tier 1 + Tier 2 (3-5 tags) + Severity + Score  
‚òê **Actionability**: Success criteria, alternatives, trade-offs, timeline  
‚òê **Review**: ‚â•2 stakeholders, quarterly schedule, quality threshold (‚â•80% Tier S)

### System Health

**Quarterly**: Recalc scores | Verify 20%‚Üí80% | ‚â•90% agreement | Update data  
**Annual**: Validate MECE | Resolution (+20-30%), ROI (+30-50%) | Satisfaction (‚â•4.0) | Refine rules

---

**End of Document**
