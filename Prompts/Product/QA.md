# Interview Q&A Generator - Product Manager

**Purpose**: Generate interview Q&A banks testing senior/director/VP-level Product Manager judgment across strategy, discovery, prioritization, metrics, stakeholder management, and go-to-market.

**Deliverable**: 25–30 scenario-based Q&A pairs with multi-dimensional evaluation, structured references, visual artifacts, and quantitative validation.

---

## I. Context & Scope

### A. Definitions & Terminology

**Core Terms**:
- **Q&A**: Question–answer pair testing PM judgment through realistic scenarios
- **Floor (≥)**: Minimum threshold that must be met or exceeded
- **Validation step**: Quantifiable pass/fail checkpoint with specific criteria
- **Quality gate**: Mandatory stop-and-fix checkpoint before proceeding to next phase
- **Difficulty levels**: Foundational (F) = execution focus; Intermediate (I) = strategy/trade-offs; Advanced (A) = portfolio/vision/P&L
- **Evaluation dimensions**: Product (user value, adoption), Business (revenue, economics), Strategic (market trends, competition), Operational (cross-functional execution, risk)

### B. Scope Boundaries

**In Scope**:
- Scenario-based questions requiring judgment, trade-off analysis, and multi-dimensional thinking
- Senior/Director/VP-level PM competencies (not junior/associate PM basics)
- Real-world dilemmas involving stakeholder tension, resource constraints, strategic ambiguity
- Cross-functional contexts (engineering, design, sales, marketing, leadership)

**Out of Scope**:
- Trivia/recall questions ("What is AARRR?")
- Hypothetical greenfield product design ("Design Instagram from scratch")
- Junior-level execution tasks without strategic complexity
- Domain-specific technical knowledge unrelated to PM judgment

---

## II. Requirements & Specifications

### A. Q&A Set Composition

**Quantitative Requirements**:
- **Total count**: 25–30 Q&A pairs
- **Difficulty distribution**: 20% F / 40% I / 40% A (e.g., 6F/12I/12A for 30 items; ±5% tolerance)
- **Answer length**: 150–300 words per answer (measured after removing artifacts)
- **Citation density**: ≥70% answers have ≥1 citation; ≥30% have ≥2 citations

**Content Coverage (MECE - Mutually Exclusive, Collectively Exhaustive)**:
1. Strategy & Vision (5–6 Q&A)
2. Discovery & User Research (4–5 Q&A)
3. Prioritization & Roadmapping (5–6 Q&A)
4. Metrics & Analytics (4–5 Q&A)
5. Stakeholder Management & Communication (4–5 Q&A)
6. Go-to-Market & Growth (4–5 Q&A)

**Multi-Dimensional Evaluation** (each answer must address ≥2 dimensions):
- **Product**: User value, feature adoption, product-market fit, user journey optimization
- **Business**: Revenue impact, market share, unit economics, pricing, strategic positioning
- **Strategic**: Market trends, competitive dynamics, regulatory factors, platform evolution, ecosystem effects
- **Operational**: Cross-functional alignment, execution velocity, resource allocation, risk mitigation, technical debt

### B. Reference & Evidence Requirements

**Minimum Reference Floors** (Must be met before proceeding to Q&A generation):
- **Glossary entries**: ≥10 (include: RICE, AARRR, JTBD, North Star Metric, PMF, OKR, Continuous Discovery, PLG, Feature Factory, OST, HEART, Value/Effort Matrix, KANO, V2MOM, Dual-track Agile)
- **Tools & Platforms**: ≥5 (must span: analytics, roadmapping, research, collaboration, feedback aggregation)
- **Authoritative Literature**: ≥6 books/frameworks/case studies (must include ≥2 ZH sources: 俞军, 梁宁, 苏杰, or equivalent)
- **APA Citations**: ≥12 credible sources with language tags ([EN], [ZH], etc.)

**Tool Documentation Requirements** (for each tool):
- Pricing tier (freemium/starter/enterprise)
- User base size or notable customers
- Last update ≤18 months from document creation date
- Key integrations (≥3)
- Primary PM use case

**Visual Artifacts** (≥1 diagram + ≥1 table per topic cluster):
- User journey maps
- Prioritization matrices (2×2, weighted scoring)
- Metric dashboards (retention cohorts, funnel conversion, activation rates)
- Roadmap timelines (now/next/later, quarterly)

**Scaling Rule**: For >30 Q&A, multiply reference floors by 1.5× (round up); all quality gates must pass before scaling.

### C. Citation & Source Standards

**Language Distribution** (Multilingual balance for global PM practice):
- **English (EN)**: 50–70% (target: 60%)
- **Chinese (ZH)**: 20–40% (target: 30%)
- **Other**: 5–15% (target: 10%)
- **Validation**: Calculate actual percentages; FAIL if outside ranges

**Source Type Diversity** (≥3 types required):
1. **Product frameworks**: RICE, JTBD, OST, HEART, Jobs Theory, Continuous Discovery
2. **Research & data**: Market analyses, user studies, industry benchmarks, quantitative studies
3. **Case studies**: Product launches, turnarounds, transformations, postmortems
4. **Tools & platforms**: Analytics (Mixpanel, Amplitude), roadmapping (ProductBoard, Aha!), research (Dovetail, UserTesting)

**Citation Format**:
- **APA 7th edition** with mandatory language tag: `Author, A. (Year). *Title*. Publisher. [EN]`
- **Inline citations**: `[Ref: ID]` immediately after factual claims, metrics, frameworks, or criteria
- **Reference ID taxonomy**:
  - G# = Glossary term (e.g., G2 = RICE)
  - T# = Tool/platform (e.g., T1 = Mixpanel)
  - L# = Literature/book (e.g., L1 = Cagan's *Inspired*)
  - A# = APA source citation (e.g., A1 = full APA entry)

**Citation Coverage** (Minimum thresholds):
- ≥70% answers contain ≥1 citation
- ≥30% answers contain ≥2 citations
- 100% framework mentions linked to [Ref: G#] or [Ref: A#]

### D. Quality Gates (Mandatory Stop Points)

**CRITICAL**: Each gate must show PASS before proceeding. If ANY gate shows FAIL, stop, fix issues, regenerate affected sections, and re-validate.

**Gate 1 – Recency**:
- ≥50% citations from last 3 years (≥70% for AI/ML/platform/data domains)
- Count publication years; calculate percentage
- **FAIL action**: Replace outdated sources with recent equivalents

**Gate 2 – Source Diversity**:
- ≥3 source types represented (frameworks, research, case studies, tools)
- No single source type >25% of total citations
- **FAIL action**: Add sources from underrepresented types

**Gate 3 – Per-Topic Evidence**:
- Each topic cluster (6 total) must have ≥2 authoritative sources + ≥1 tool reference
- **FAIL action**: Add missing references for deficient topics

**Gate 4 – Tool Completeness**:
- Every tool entry includes: pricing tier, user base/customers, last update date (≤18 months), ≥3 integrations
- **FAIL action**: Research and add missing tool details

**Gate 5 – Link Validation**:
- All URLs accessible or archived (use Wayback Machine for deprecated links)
- Prefer DOIs for academic sources
- **FAIL action**: Fix broken links, archive unavailable pages, or replace sources

**Gate 6 – Cross-Reference Integrity**:
- 100% of inline [Ref: ID] citations resolve to Reference Sections entries
- No orphaned references (entries without citations)
- **FAIL action**: Add missing entries or remove invalid [Ref: ID] tags

---

## III. Step-by-Step Instructions

### Step 1: Topic Planning & Allocation

**Objective**: Create balanced coverage across 6 PM competency areas with correct difficulty distribution.

**Actions**:
1. **Identify 6 topic clusters** from Content Coverage (Section II.A):
   - Strategy & Vision
   - Discovery & User Research
   - Prioritization & Roadmapping
   - Metrics & Analytics
   - Stakeholder Management & Communication
   - Go-to-Market & Growth

2. **Allocate Q&A per cluster**: Distribute 25–30 total questions (aim for 4–6 per cluster)
   - Example for 30 Q&A: 5, 5, 6, 5, 4, 5 across clusters

3. **Assign difficulty labels**: Apply 20% F / 40% I / 40% A distribution
   - Example for 30 Q&A: 6F, 12I, 12A
   - Spread evenly: each cluster should have ≥1F, ≥1I, ≥1A

**Self-Check**:
- [ ] Total Q&A = 25–30
- [ ] Sum of F+I+A = total Q&A
- [ ] F% ≈ 20% (±5%), I% ≈ 40% (±5%), A% ≈ 40% (±5%)
- [ ] Each cluster has ≥1 question from each difficulty level

**Output**: Populate Topic Areas Overview table (see Output Format Template)

### Step 2: Reference Collection & Documentation

**Objective**: Build authoritative reference foundation meeting all quantitative floors and quality gates BEFORE generating Q&A.

**Actions**:

**2.1 Glossary (≥10 entries)**:
- **Required terms**: RICE, AARRR, JTBD, North Star Metric, PMF, OKR, Continuous Discovery, PLG, Feature Factory, OST
- **Recommended additions**: HEART, Value/Effort Matrix, KANO, V2MOM, Dual-track Agile, ICE Score, Opportunity Solution Tree, Activation Rate, Retention Cohorts
- **Format**: Term name, definition (1–2 sentences), use cases, related terms
- **Assign IDs**: G1, G2, G3...

**2.2 Tools & Platforms (≥5 entries)**:
- **Analytics**: Mixpanel, Amplitude, Heap, Google Analytics 4
- **Roadmapping**: ProductBoard, Aha!, Productplan, Roadmunk
- **Research**: Dovetail, UserTesting, Maze, Optimal Workshop
- **Collaboration**: Miro, Figma (FigJam), Notion, Confluence
- **Feedback**: Productboard (feedback aggregation), Pendo, Canny
- **Required fields**: Category, pricing, user base/customers, last update (≤18 months), integrations (≥3), PM use case, URL
- **Assign IDs**: T1, T2, T3...

**2.3 Authoritative Literature (≥6 entries)**:
- **EN sources**: Cagan (*Inspired*), Olsen (*Lean Product Playbook*), Torres (*Continuous Discovery*), Perri (*Escaping the Build Trap*), Patton (*User Story Mapping*), Klement (*Jobs to Be Done*)
- **ZH sources** (≥2 required): 俞军 (*俞军产品方法论*), 梁宁 (*产品思维30讲*), 苏杰 (*人人都是产品经理*)
- **Format**: Author, title, year, brief summary (focus area), key frameworks
- **Assign IDs**: L1, L2, L3...

**2.4 APA Citations (≥12 entries)**:
- Convert all Glossary, Tools, Literature sources to APA 7th edition format
- Add language tags: [EN], [ZH], [ES], etc.
- Include publication year, verify recency (≥50% from last 3 years)
- Classify by source type (1=frameworks, 2=research, 3=case studies, 4=tools)
- **Assign IDs**: A1, A2, A3... (may map to L# or T# entries)

**Self-Check**:
- [ ] Glossary ≥10, Tools ≥5, Literature ≥6, APA ≥12
- [ ] Language distribution: EN 50–70%, ZH 20–40%, Other 5–15%
- [ ] Recency: ≥50% from last 3 years (calculate using APA years)
- [ ] Source diversity: ≥3 types represented, no type >25%
- [ ] All tools updated ≤18 months ago
- [ ] All IDs assigned (G#, T#, L#, A#)

**Quality Gate Checkpoint**: Run Gates 1–6 (Section II.D). If FAIL, fix and re-validate before Step 3.

### Step 3: Q&A Generation

**Objective**: Create scenario-based questions with structured answers demonstrating PM judgment, multi-dimensional thinking, and evidence-based reasoning.

**3.1 Question Design Principles**:

**Format Requirements**:
- **Scenario-based**: "How would you..." / "Walk me through..." / "Your CEO wants... what do you do?"
- **Avoid recall**: NOT "What is RICE?" / "List the 5 AARRR stages" / "Define North Star Metric"
- **Single unambiguous ask**: One clear decision or analysis required
- **Realistic context**: Include constraints (time, resources, stakeholder pressure, conflicting data)

**Judgment Signals** (questions must test ≥2 of these):
- Trade-off analysis (revenue vs. vision, short-term vs. long-term)
- Opportunity cost evaluation (choosing between A, B, C with limited resources)
- Stakeholder tension navigation (sales vs. engineering, CEO pressure vs. product principles)
- Incomplete information handling (ambiguous metrics, conflicting user feedback)
- Execution complexity assessment (sequencing, dependencies, risk mitigation)

**Seniority Alignment**:
- **Foundational (F)**: Execution-focused ("How do you track activation?", "Run this A/B test analysis")
- **Intermediate (I)**: Strategy/trade-offs ("Prioritize churn reduction vs. new features", "Sales wants custom work—decide")
- **Advanced (A)**: Portfolio/vision/P&L ("Choose: platform API, mobile app, or intl expansion", "Pivot to PLG—what changes?")

**3.2 Answer Structure** (mandatory components):

**Template**:
1. **Key Insight** (1 sentence): State the specific dilemma/tension this question exposes
   - Examples: "Tests tension between revenue protection and PMF", "Reveals bias toward vanity metrics over business outcomes"

2. **Answer Body** (150–300 words):
   - **Framework/approach**: Name the method(s) you'd use [Ref: G#/A#]
   - **Multi-dimensional analysis**: Address ≥2 evaluation dimensions (Product, Business, Strategic, Operational)
   - **Concrete steps**: What you'd do first, second, third (sequenced actions)
   - **Trade-offs**: Explicitly name what you're optimizing for and what you're sacrificing
   - **Stakeholder communication**: How you'd present the decision/analysis
   - **Success criteria**: How you'd measure if the decision was right

3. **Citations**: Include ≥1 [Ref: ID] inline after frameworks, metrics, tools, or factual claims

4. **Supporting Artifact** (if applicable): Decision matrix, user journey, metric dashboard, roadmap

**3.3 Generation Process**:
1. Write 5 Q&A at a time (one cluster or portion)
2. Run self-check after each batch
3. Proceed to next batch only after passing self-check

**Self-Check (per 5 Q&A batch)**:
- [ ] All questions scenario-based ("How would..." not "What is...")
- [ ] All questions test judgment ≥2 signals (trade-offs, opportunity cost, stakeholder tension, etc.)
- [ ] All answers 150–300 words (excluding artifacts)
- [ ] All answers include Key Insight (concrete dilemma/tension stated)
- [ ] All answers address ≥2 evaluation dimensions
- [ ] ≥3/5 answers have ≥1 citation; ≥1/5 has ≥2 citations
- [ ] No recall/trivia questions ("Define X", "List Y")
- [ ] Difficulty labels match content (F=execution, I=strategy, A=portfolio/vision)

### Step 4: Visual Artifacts Creation

**Objective**: Provide visual decision-support tools demonstrating structured PM thinking.

**Requirements**: ≥1 diagram + ≥1 table per topic cluster (6 clusters total = ≥6 diagrams + ≥6 tables)

**Artifact Types by Topic**:

**Strategy & Vision**:
- **Diagram**: Product vision roadmap (now/next/later), competitive positioning map (2×2)
- **Table**: Strategic decision matrix (weighted scoring across criteria)

**Discovery & User Research**:
- **Diagram**: User journey map (stages, pain points, opportunities), Jobs-to-be-Done map
- **Table**: Research synthesis matrix (insight, evidence, impact, action)

**Prioritization & Roadmapping**:
- **Diagram**: Opportunity Solution Tree, Value vs. Effort matrix (2×2)
- **Table**: RICE scoring table, prioritization criteria comparison

**Metrics & Analytics**:
- **Diagram**: Funnel conversion diagram (AARRR stages with rates), retention cohort chart
- **Table**: Metric dashboard (KPI, current, target, trend, owner)

**Stakeholder Management**:
- **Diagram**: Stakeholder map (power vs. interest 2×2), communication plan timeline
- **Table**: Stakeholder matrix (name, interest, concern, engagement strategy)

**Go-to-Market & Growth**:
- **Diagram**: Growth loop diagram, channel strategy map
- **Table**: GTM launch checklist, channel performance comparison

**Format Guidelines**:
- Use Markdown tables (pipe-delimited) for structured data
- Use ASCII diagrams, Mermaid syntax, or descriptive text for visual diagrams
- Ensure artifacts directly support Q&A answers (reference from answer text)
- Include: column/row headers, units/percentages, clear labels

**Self-Check**:
- [ ] All 6 topic clusters have ≥1 diagram
- [ ] All 6 topic clusters have ≥1 table
- [ ] All artifacts include clear labels/headers
- [ ] ≥50% artifacts referenced from Q&A answer text

### Step 5: Reference Sections Population

**Objective**: Complete all 4 reference sections with full details per Output Format Template.

**Actions**:

**5.1 Glossary, Terminology & Acronyms**:
- Format: **G#. Term Name (Acronym)**
  - Definition (1–2 sentences)
  - Use cases (when to apply)
  - Related terms (cross-references to other G# entries)
- Alphabetize by term name
- Include phonetic/translation for non-EN terms if helpful

**5.2 Product Tools & Platforms**:
- Format: **T#. Tool Name (Category)**
  - Description (key features, 1–2 sentences)
  - Pricing (Freemium / $X/user/mo / Enterprise)
  - User base ("8K+ companies" or notable customers: Uber, Netflix)
  - Last update (Q# YYYY)
  - Integrations (list ≥3: Segment, Salesforce, Slack, Jira, etc.)
  - PM use case (activation tracking, roadmap communication, etc.)
  - URL
- Group by category (Analytics, Roadmapping, Research, Collaboration, Feedback)

**5.3 Authoritative Literature & Case Studies**:
- Format: **L#. Author, Title, Year**
  - Brief summary (focus area, key frameworks, 1–2 sentences)
  - Relevance to PM practice
- Group by region/language (EN sources, ZH sources, Other)
- Include ≥2 ZH sources

**5.4 APA Style Source Citations**:
- Format: **A#. APA 7th edition citation [Language Tag]**
  - For books: Author, A. (Year). *Title* (Edition). Publisher. [EN]
  - For articles: Author, A. (Year). Title. *Journal*, Volume(Issue), pages. https://doi.org/... [EN]
  - For web: Author/Org. (Year, Month Day). *Title*. Site Name. URL [EN]
- Include romanized version for ZH sources: 俞军. (2020). *俞军产品方法论*. 中信出版社. [ZH] (Yu, J. (2020). *Yu Jun's Product Methodology*. CITIC Press.)
- Sort by ID (A1, A2, A3...)

**Self-Check**:
- [ ] All [Ref: G#] in Q&A resolve to Glossary entries
- [ ] All [Ref: T#] in Q&A resolve to Tools entries
- [ ] All [Ref: L#] in Q&A resolve to Literature entries
- [ ] All [Ref: A#] in Q&A resolve to APA Citations entries
- [ ] No orphaned references (entries not cited in Q&A)
- [ ] All required fields populated per format above
- [ ] Language tags present on all APA citations

### Step 6: Comprehensive Validation

**Objective**: Execute all 12 validation steps and generate quantitative pass/fail report.

**Process**:
1. Run validation checks 1–12 (Section IV.A - Validation Steps)
2. Record results in Validation Report Table (Section IV.B)
3. **If ANY check shows FAIL**:
   - Stop generation
   - Identify root cause (missing references, incorrect distribution, broken links, etc.)
   - Fix affected sections
   - Re-run ALL 12 validation steps (not just failed ones)
   - Repeat until all 12 show PASS
4. Proceed to Step 7 only when all checks PASS

**Documentation**: Include completed Validation Report table in final output (see Section IV.B for template)

---

### Step 7: Final Review & Quality Assurance

**Objective**: Human review of question quality and submission readiness.

**7.1 Question Design Critique**:
Review all questions against criteria in Section V (Question Design Critique Criteria):
- [ ] **Clarity**: Single unambiguous ask (not multi-part)
- [ ] **Signal**: Tests judgment, not trivia
- [ ] **Depth**: Enables trade-off/opportunity cost discussion
- [ ] **Realism**: Matches senior/director/VP PM roles
- [ ] **Discriminative**: Tests judgment over recall
- [ ] **Alignment**: Difficulty matches seniority (F=execution, I=strategy, A=vision/P&L)

**7.2 Answer Quality Review** (sample ≥5 answers):
- [ ] Multi-dimensional analysis (≥2 dimensions addressed)
- [ ] Concrete steps/frameworks named
- [ ] Trade-offs explicitly stated
- [ ] Citations support claims
- [ ] Success criteria or metrics included

**7.3 Submission Checklist**:
- [ ] All 12 validation steps show PASS (Validation Report included)
- [ ] All reference floors met (G≥10, T≥5, L≥6, A≥12)
- [ ] All 6 quality gates passed (Section II.D)
- [ ] Table of Contents with working anchor links
- [ ] All sections follow Output Format Template (Section VI)
- [ ] No placeholder text ("[TODO]", "[INSERT]", etc.)
- [ ] Consistent formatting (headings, lists, tables)

**Final Action**: If all checks pass, mark document ready for use. If not, return to relevant step and fix.

---

## IV. Validation & Quality Gates

**CRITICAL REQUIREMENT**: Execute ALL 12 validation steps. If ANY check shows FAIL, immediately stop, fix root cause, regenerate affected sections, and re-run ALL validations (not just failed ones). Only proceed when ALL 12 checks show PASS.

### A. Validation Steps (12-Point Checklist)

**Step 1 – Quantitative Floors**:
- **Measurement**:
  - Count Glossary entries: Must be ≥10
  - Count Tools entries: Must be ≥5
  - Count Literature entries: Must be ≥6
  - Count APA Citations entries: Must be ≥12
  - Count total Q&A: Must be 25–30
  - Calculate difficulty distribution: Count F, I, A; compute percentages
- **Pass Criteria**: All floors met + difficulty distribution 20%F / 40%I / 40%A (±5% tolerance)
- **Fail Action**: Add missing entries or adjust Q&A allocation

**Step 2 – Citation Coverage**:
- **Measurement**:
  - Count answers with ≥1 citation: Calculate percentage of total
  - Count answers with ≥2 citations: Calculate percentage of total
- **Pass Criteria**: ≥70% have ≥1 citation AND ≥30% have ≥2 citations
- **Fail Action**: Add [Ref: ID] tags to under-cited answers (prioritize factual claims, frameworks, metrics)

**Step 3 – Language Distribution**:
- **Measurement**:
  - Count [EN] tags in APA Citations: Calculate %
  - Count [ZH] tags in APA Citations: Calculate %
  - Count other language tags: Calculate %
- **Pass Criteria**: EN 50–70%, ZH 20–40%, Other 5–15%
- **Fail Action**: Replace sources to rebalance distribution

**Step 4 – Recency Check**:
- **Measurement**:
  - Extract publication year from each APA citation
  - Count citations from (current year − 3) or later
  - Calculate percentage
  - Identify domain: if AI/ML/platform/data mentioned ≥3 times, apply 70% threshold
- **Pass Criteria**: ≥50% from last 3 years (or ≥70% for AI/platform domains)
- **Fail Action**: Replace outdated sources with recent equivalents (search for "2022", "2023", "2024" versions)

**Step 5 – Source Type Diversity**:
- **Measurement**:
  - Tag each APA citation by type: (1) Frameworks, (2) Research/Data, (3) Case Studies, (4) Tools
  - Count unique types represented
  - Calculate percentage of each type: (count of type / total citations) × 100
- **Pass Criteria**: ≥3 types present AND no single type >25%
- **Fail Action**: Add sources from underrepresented types

**Step 6 – Link Accessibility**:
- **Measurement**:
  - Extract all URLs from Tools and APA Citations sections
  - Test each URL (HTTP status 200 or archived version exists)
  - Count accessible links / total links
- **Pass Criteria**: 100% accessible or archived (use Wayback Machine for deprecated)
- **Fail Action**: Fix broken links, archive pages, or replace sources with accessible alternatives

**Step 7 – Cross-Reference Integrity**:
- **Measurement**:
  - Extract all [Ref: G#/T#/L#/A#] from Q&A answers
  - Verify each ID exists in corresponding Reference Section
  - Count resolved references / total references
- **Pass Criteria**: 100% resolved (no dangling [Ref: XX] tags)
- **Fail Action**: Add missing reference entries or remove invalid [Ref: ID] tags

**Step 8 – Answer Word Count**:
- **Measurement**:
  - Randomly sample 5 answers (or all if <10 total)
  - Count words in answer body (exclude Key Insight, artifacts, metadata)
  - Verify each is 150–300 words
- **Pass Criteria**: 100% of sampled answers within 150–300 word range
- **Fail Action**: Expand short answers (add frameworks, trade-offs, steps) or condense long answers (remove redundancy)

**Step 9 – Key Insight Quality**:
- **Measurement**:
  - Review all Key Insights
  - Classify as: (a) Concrete (names specific dilemma/tension), (b) Vague (generic statement)
  - Count concrete insights / total insights
- **Pass Criteria**: 100% concrete (must state specific user impact, business trade-off, prioritization dilemma, or stakeholder tension)
- **Fail Action**: Rewrite vague insights to name specific PM judgment challenge
  - ❌ "Tests prioritization skills"
  - ✓ "Tests tension between short-term revenue (enterprise customization) and long-term PMF (mass-market scalability)"

**Step 10 – Per-Topic Evidence Floors**:
- **Measurement**:
  - For each of 6 topic clusters, count:
    - Authoritative sources (L# or A# citations) mentioned in that cluster's Q&A
    - Tool references (T# citations) mentioned in that cluster's Q&A
  - Verify: ≥2 authoritative sources AND ≥1 tool per cluster
- **Pass Criteria**: All 6 clusters meet minimums
- **Fail Action**: Add [Ref: L#/A#/T#] citations to deficient clusters

**Step 11 – Framework Usage Accuracy**:
- **Measurement**:
  - Identify all PM frameworks mentioned (RICE, JTBD, OST, AARRR, etc.)
  - Verify each includes:
    - [Ref: G#] citation linking to Glossary
    - Correct description/application
    - Acknowledgment of limitations or context (when applicable)
  - Count correct framework usages / total framework mentions
- **Pass Criteria**: ≥80% correct + cited + limitations acknowledged (where relevant)
- **Fail Action**: Add [Ref: G#] tags, fix incorrect descriptions, note "limitations: X" for frameworks with known blind spots

**Step 12 – Judgment vs. Recall Ratio**:
- **Measurement**:
  - Classify each question as:
    - (a) Judgment: Scenario-based ("How would...", "Walk me through...", "Your CEO wants...")
    - (b) Recall: Definitional ("What is...", "List...", "Define...")
  - Calculate: judgment questions / total questions × 100
- **Pass Criteria**: ≥70% judgment-based
- **Fail Action**: Replace recall questions with scenario-based equivalents
  - ❌ "What is RICE prioritization?"
  - ✓ "When would RICE prioritization mislead you? How would you supplement it?"

### B. Validation Report Template

**MANDATORY**: Complete this table after running all 12 validation steps. Include in final output.

```markdown
| Step | Check                    | Measurement                                      | Pass Criteria                        | Result              | Status     |
|------|--------------------------|--------------------------------------------------|--------------------------------------|---------------------|------------|
| 1    | Quantitative Floors      | G:___ T:___ L:___ A:___ Q:___ (___F/___I/___A)  | G≥10, T≥5, L≥6, A≥12, Q:25-30, 20/40/40% | [Fill actual counts] | PASS/FAIL  |
| 2    | Citation Coverage        | ___% have ≥1 cite, ___% have ≥2 cites           | ≥70% ≥1 cite, ≥30% ≥2 cites        | [Fill %]            | PASS/FAIL  |
| 3    | Language Distribution    | EN:__%, ZH:__%, Other:__%                        | EN:50-70%, ZH:20-40%, Other:5-15%    | [Fill %]            | PASS/FAIL  |
| 4    | Recency                  | ___% from last 3 years (domain: _____)           | ≥50% (≥70% if AI/platform)          | [Fill %]            | PASS/FAIL  |
| 5    | Source Diversity         | ___ types present; max type = ___%               | ≥3 types, no type >25%              | [Fill counts]       | PASS/FAIL  |
| 6    | Link Accessibility       | ___/___ links accessible                         | 100% accessible or archived          | [Fill fraction]     | PASS/FAIL  |
| 7    | Cross-Reference Integrity| ___/___ [Ref: ID] tags resolved                  | 100% resolved                        | [Fill fraction]     | PASS/FAIL  |
| 8    | Answer Word Count        | Sampled ___ answers: ___ compliant               | 100% within 150-300 words            | [Fill counts]       | PASS/FAIL  |
| 9    | Key Insight Quality      | ___/___ concrete insights                        | 100% concrete                        | [Fill fraction]     | PASS/FAIL  |
| 10   | Per-Topic Evidence       | ___/6 clusters meet minimums (≥2 auth + ≥1 tool) | 6/6 clusters                         | [Fill count]        | PASS/FAIL  |
| 11   | Framework Usage          | ___/___ frameworks correct + cited + limitations | ≥80% correct                         | [Fill fraction]     | PASS/FAIL  |
| 12   | Judgment vs Recall       | ___% judgment-based (scenario) questions         | ≥70% judgment-based                 | [Fill %]            | PASS/FAIL  |
```

**Instructions**:
1. Fill in all "___" placeholders with actual measurements
2. Mark Status as PASS or FAIL based on Pass Criteria
3. If ANY row shows FAIL: stop, fix issues, regenerate, re-run ALL 12 steps
4. Only proceed when ALL 12 rows show PASS

### C. Submission Checklist

**Pre-Submission Requirements** (ALL must be checked before marking document complete):

**Validation**:
- [ ] All 12 validation steps show PASS in Validation Report table
- [ ] Validation Report table included in final output with actual measurements (no "___" placeholders)

**Reference Floors**:
- [ ] Glossary ≥10 entries (all with ID, definition, use cases, related terms)
- [ ] Tools ≥5 entries (all with pricing, user base, update date, integrations, URL)
- [ ] Literature ≥6 entries (all with summary, relevance)
- [ ] APA Citations ≥12 entries (all with language tags, correct APA 7th format)

**Quality Gates** (Section II.D):
- [ ] Gate 1: Recency ≥ thresholds
- [ ] Gate 2: Source diversity ≥3 types, no type >25%
- [ ] Gate 3: Per-topic evidence floors met (all 6 clusters)
- [ ] Gate 4: All tools have complete details
- [ ] Gate 5: All links accessible/archived
- [ ] Gate 6: All [Ref: ID] tags resolve (100%)

**Content Completeness**:
- [ ] Table of Contents with working anchor links to all sections
- [ ] Topic Areas Overview table populated
- [ ] All 25–30 Q&A follow template format (Difficulty, Type, Key Insight, Answer, Artifact)
- [ ] ≥1 diagram + ≥1 table per topic cluster (6 clusters = ≥6 diagrams + ≥6 tables)
- [ ] All 4 Reference Sections fully populated (Glossary, Tools, Literature, APA)

**Quality Assurance**:
- [ ] No placeholder text ("[TODO]", "[INSERT]", "[TBD]", "___")
- [ ] Consistent formatting (headings, lists, tables, code blocks)
- [ ] All questions scenario-based (no "What is..." / "List..." / "Define..." trivia)
- [ ] All Key Insights concrete (name specific dilemma/tension)
- [ ] All answers 150–300 words (excluding artifacts)
- [ ] All framework mentions cited with [Ref: G#] or [Ref: A#]

**Final Action**: If all boxes checked, document is ready for use. If not, return to relevant section and complete missing requirements.

---

## V. Question Design Critique Criteria

**Purpose**: Review questions for PM judgment rigor before finalizing output.

### Quality Dimensions

**1. Clarity (Single Unambiguous Ask)**:
- **Pass**: One clear decision or analysis required; no multi-part questions
- **Fail**: Combines multiple unrelated topics or requires answering A, B, and C
- **Examples**:
  - ✓ "How would you prioritize between improving activation rate and reducing churn?"
  - ❌ "Explain retention metrics and database optimization strategies" (two unrelated topics)

**2. Signal (Tests Judgment, Not Trivia)**:
- **Pass**: Requires PM judgment, trade-off analysis, or contextual application of frameworks
- **Fail**: Tests memorization or definitional knowledge
- **Examples**:
  - ✓ "CEO wants AI features in the product. How would you approach this request?"
  - ❌ "List the five stages of the AARRR framework" (pure recall)

**3. Depth (Enables Trade-Off & Opportunity Cost Discussion)**:
- **Pass**: Question structure allows exploring multiple options, constraints, second-order effects
- **Fail**: Binary yes/no or single-path answer
- **Examples**:
  - ✓ "Choose one: build platform API, launch mobile app, or expand internationally. How do you decide?"
  - ❌ "Should you build a mobile app? Yes or no." (no depth)

**4. Realism (Matches Senior/Director/VP PM Roles)**:
- **Pass**: Scenario reflects actual senior+ PM dilemmas with stakeholder pressure, incomplete data, resource constraints
- **Fail**: Hypothetical academic exercise or junior IC task
- **Examples**:
  - ✓ "Sales team needs 3 custom features for a $5M deal. Engineering says it derails the roadmap. What do you do?"
  - ❌ "Design Instagram from scratch in 45 minutes" (whiteboard exercise, not realistic judgment scenario)

**5. Discriminative (Tests Judgment Over Recall)**:
- **Pass**: Requires applying knowledge to novel situations; reveals thinking process
- **Fail**: Can be answered by repeating textbook definitions
- **Examples**:
  - ✓ "When would RICE prioritization mislead you? How would you supplement it?"
  - ❌ "What does RICE stand for?" (acronym recall)

**6. Seniority Alignment (Difficulty ↔ Role Level)**:
- **Foundational (F)**: Execution-focused, single-team scope
  - "How would you track activation rate improvements after a new onboarding flow?"
- **Intermediate (I)**: Strategy/trade-offs, cross-functional coordination
  - "Should you prioritize reducing churn or improving new user activation? Walk through your analysis."
- **Advanced (A)**: Portfolio/vision/P&L, org-wide impact, strategic pivots
  - "Your company needs to choose: build a platform API, launch mobile app, or expand internationally. You have resources for one. How do you decide?"

### Review Process

**Checklist** (Apply to each question):
- [ ] Passes Clarity test (single ask)
- [ ] Passes Signal test (judgment, not trivia)
- [ ] Passes Depth test (enables trade-off discussion)
- [ ] Passes Realism test (senior+ PM scenario)
- [ ] Passes Discriminative test (application, not recall)
- [ ] Difficulty label matches seniority (F=execution, I=strategy, A=portfolio/vision)

**Revision**: If a question fails ≥2 criteria, rewrite or replace.

---

## VI. Output Format Template

**Instructions**: Use this exact structure when generating final Q&A bank deliverable. All sections are mandatory.

### A. Table of Contents Template

**Structure**: Use Markdown anchor links for navigation. All links must resolve to corresponding sections.

```markdown
## Table of Contents

1. [Topic Areas Overview](#topic-areas-overview)
2. [Questions by Topic](#questions-by-topic)
   - [Topic 1: Strategy & Vision](#topic-1-strategy--vision)
     - [Q1: [Abbreviated question text]](#q1-abbreviated-question)
     - [Q2: [Abbreviated question text]](#q2-abbreviated-question)
   - [Topic 2: Discovery & User Research](#topic-2-discovery--user-research)
     - [Q3: [Abbreviated question text]](#q3-abbreviated-question)
   - [Topic 3: Prioritization & Roadmapping](#topic-3-prioritization--roadmapping)
   - [Topic 4: Metrics & Analytics](#topic-4-metrics--analytics)
   - [Topic 5: Stakeholder Management & Communication](#topic-5-stakeholder-management--communication)
   - [Topic 6: Go-to-Market & Growth](#topic-6-go-to-market--growth)
3. [Reference Sections](#reference-sections)
   - [Glossary, Terminology & Acronyms](#glossary-terminology--acronyms)
   - [Product Tools & Platforms](#product-tools--platforms)
   - [Authoritative Literature & Case Studies](#authoritative-literature--case-studies)
   - [APA Style Source Citations](#apa-style-source-citations)
4. [Validation Report](#validation-report)
```

### B. Topic Areas Overview Template

**Purpose**: Provide at-a-glance summary of Q&A distribution and difficulty balance.

```markdown
## Topic Areas Overview

**Total Q&A**: [25–30]
**Difficulty Distribution**: [X]F ([Y]%) / [X]I ([Y]%) / [X]A ([Y]%)
**Coverage**: 6 PM competency areas (MECE)

| #  | Topic Cluster                        | Question Range | Count | Difficulty Mix | Artifacts           |
|----|--------------------------------------|----------------|-------|----------------|---------------------|
| 1  | Strategy & Vision                    | Q1–Q5          | 5     | 1F, 2I, 2A     | 1 diagram, 1 table  |
| 2  | Discovery & User Research            | Q6–Q10         | 5     | 1F, 2I, 2A     | 1 diagram, 1 table  |
| 3  | Prioritization & Roadmapping         | Q11–Q16        | 6     | 1F, 2I, 3A     | 1 diagram, 1 table  |
| 4  | Metrics & Analytics                  | Q17–Q21        | 5     | 1F, 2I, 2A     | 1 diagram, 1 table  |
| 5  | Stakeholder Management & Communication | Q22–Q25      | 4     | 1F, 2I, 1A     | 1 diagram, 1 table  |
| 6  | Go-to-Market & Growth                | Q26–Q30        | 5     | 1F, 2I, 2A     | 1 diagram, 1 table  |
|    | **Total**                            |                | **30**| **6F, 12I, 12A** | **6 diagrams, 6 tables** |

**Legend**:
- **F** = Foundational (execution-focused, single-team scope)
- **I** = Intermediate (strategy, trade-offs, cross-functional coordination)
- **A** = Advanced (portfolio/vision/P&L, org-wide strategic decisions)
```

### C. Question & Answer Template

**Structure**: Each question follows this exact format.

```markdown
## Topic 1: [Topic Title]

### Q1: [Full Question Text]

**Difficulty**: [Foundational / Intermediate / Advanced]
**Topic Area**: [Strategy & Vision / Discovery & User Research / Prioritization & Roadmapping / Metrics & Analytics / Stakeholder Management & Communication / Go-to-Market & Growth]

**Key Insight**: [One concrete sentence naming the specific dilemma, tension, or trade-off this question exposes. Must state user impact, business trade-off, prioritization challenge, or stakeholder conflict.]

---

**Answer**:

[150–300 word structured answer addressing:]
- **Framework/Approach**: Name the method(s) you'd use with [Ref: ID] citations
- **Multi-Dimensional Analysis**: Address ≥2 evaluation dimensions (Product, Business, Strategic, Operational)
- **Concrete Steps**: What you'd do (first, second, third)
- **Trade-Offs**: Explicitly state what you're optimizing for vs. sacrificing
- **Stakeholder Communication**: How you'd present the decision/analysis
- **Success Criteria**: How you'd measure if the decision was right

[Include ≥1 inline [Ref: ID] citation after frameworks, metrics, tools, or factual claims]

---

**Supporting Artifact** *(if applicable)*:

[Include diagram, table, or visual decision-support tool:]
- Decision matrix (weighted scoring)
- User journey map (stages, pain points, opportunities)
- Prioritization matrix (Value vs. Effort, RICE scores)
- Metric dashboard (KPIs, targets, trends)
- Roadmap timeline (now/next/later)
- Stakeholder map (power vs. interest)

[Use Markdown tables or ASCII diagrams]
```

**Example Question Structure**:

```markdown
## Topic 1: Strategy & Vision

### Q1: How would you evaluate whether to build a new feature requested by your top 5 enterprise customers (40% of revenue) that doesn't align with your product vision for the mass market?

**Difficulty**: Advanced
**Topic Area**: Strategy & Vision, Prioritization & Roadmapping

**Key Insight**: Tests tension between short-term revenue protection (enterprise retention) and long-term product-market fit (mass-market scalability); distinguishes PMs who navigate executive pressure from those defaulting to either pleasing customers or rigid vision adherence.

---

**Answer**:

[150–300 word answer here...]

---

**Supporting Artifact**:

[Table/diagram here...]
```

---

### Question Design Critique Criteria

Review questions for:

- **Clarity**: Single unambiguous ask
  - ✅ "How would you prioritize between improving activation rate and reducing churn?"
  - ❌ "Explain retention metrics and database optimization strategies"

- **Signal**: Tests product judgment, not trivia
  - ✅ "CEO wants AI in the product. How would you approach this?"
  - ❌ "List the five AARRR steps"

- **Depth**: Enables discussion of trade-offs, opportunity costs, execution challenges
  - ✅ "Choose one: platform API, mobile app, or international expansion. How?"
  - ❌ "Should you build a mobile app? Yes/no"

- **Realism**: Scenarios matching senior/director/VP PM roles
  - ✅ "Sales needs 3 custom features for $5M deal. Engineering says it derails roadmap. What do you do?"
  - ❌ "Design Instagram from scratch"

- **Discriminative**: Tests judgment over recall
  - ✅ "When would RICE prioritization mislead you?"
  - ❌ "What does RICE stand for?"

- **Alignment**: Match seniority (Senior: execution | Director: strategy/portfolio | VP: vision/P&L)

### D. Reference Sections Template

**Instructions**: Populate all 4 subsections with complete details per format below.

#### D.1 Glossary, Terminology & Acronyms

**Format** (for each entry):
```markdown
**G#. Term Name (Acronym if applicable)**
[Definition in 1–2 sentences explaining what it is and core components]
Used for: [Primary use cases or application contexts]
Related: [Cross-references to other G# entries]
Limitations: [Known blind spots, when not to use] *(if applicable)*
```

**Example**:
```markdown
**G1. AARRR (Pirate Metrics)**
Acquisition → Activation → Retention → Referral → Revenue framework for tracking growth across the customer lifecycle.
Used for: SaaS growth analytics, funnel optimization, identifying leaky bucket stages
Related: HEART [Ref: G10], North Star Metric [Ref: G4]
Limitations: Focuses on metrics over user value; can incentivize vanity metrics if not tied to business outcomes

**G2. RICE Prioritization**
Reach × Impact × Confidence ÷ Effort scoring framework for feature prioritization.
Used for: Roadmap planning, backlog ranking, stakeholder alignment on trade-offs
Related: ICE Score [Ref: G12], Value/Effort Matrix [Ref: G11], KANO Model [Ref: G13]
Limitations: Quantifies subjective judgments; confidence score prone to optimism bias; doesn't capture strategic value
```

**Minimum**: ≥10 entries (alphabetize by term name)

---

#### D.2 Product Tools & Platforms

**Format** (for each entry):
```markdown
**T#. Tool Name** (Category)
[Brief description of key features in 1–2 sentences]
**Pricing**: [Freemium / $X/user/mo / Enterprise / Open-source]
**User Base**: ["8K+ companies" OR notable customers: Company A, Company B]
**Last Update**: [Q# YYYY] ([key new feature or update])
**Integrations**: [List ≥3: Tool1, Tool2, Tool3, Tool4]
**PM Use Case**: [Primary application for product managers]
**URL**: [https://...]
```

**Example**:
```markdown
**T1. Mixpanel** (Product Analytics)
Event-based analytics platform for tracking user behavior, funnel/cohort analysis, A/B testing, and behavioral segmentation.
**Pricing**: Freemium (up to 100K events/mo), Growth $25/mo, Enterprise custom
**User Base**: 8,000+ companies (Uber, Netflix, Salesforce, Samsung)
**Last Update**: Q3 2024 (AI-powered insights, predictive analytics)
**Integrations**: Segment, Salesforce, Slack, Jira, Amplitude, mParticle, Braze
**PM Use Case**: Activation rate tracking, feature adoption measurement, retention cohort analysis, funnel drop-off diagnosis
**URL**: https://mixpanel.com

**T2. ProductBoard** (Roadmapping & Prioritization)
Feedback aggregation, prioritization matrix (value/effort), roadmap visualization (now/next/later, quarterly), customer-facing portal.
**Pricing**: Essentials $25/maker/mo, Pro $69/maker/mo, Enterprise custom
**User Base**: 6,000+ teams (Microsoft, Zoom, Zendesk, UiPath)
**Last Update**: Q4 2024 (AI feedback analysis, auto-categorization)
**Integrations**: Jira, Slack, Salesforce, Intercom, Zendesk, Pendo, Zapier
**PM Use Case**: Centralizing user feedback, RICE scoring, stakeholder roadmap communication, feature request voting
**URL**: https://www.productboard.com
```

**Minimum**: ≥5 entries (group by category: Analytics, Roadmapping, Research, Collaboration, Feedback)

---

#### D.3 Authoritative Literature & Case Studies

**Format** (for each entry):
```markdown
**L#. Author(s), Title, Year**
[Brief summary of focus area and key frameworks/concepts in 1–2 sentences]
Relevance: [Why PMs should read this; what gaps it fills]
```

**Example**:
```markdown
**L1. Cagan, M. (2017). *Inspired: How to Create Tech Products Customers Love* (2nd ed.)**
Foundational PM framework distinguishing discovery (de-risking) from delivery (execution); emphasizes empowered product teams, outcome-based roadmaps, and continuous discovery.
Relevance: Gold standard for modern PM practice; establishes principles for product vision, strategy, and team autonomy.

**L2. Olsen, D. (2015). *The Lean Product Playbook: How to Innovate with MVPs and Rapid Customer Feedback***
Prescribes 6-step process for achieving product-market fit: target customer, underserved needs, value proposition, feature set, MVP, testing/iteration.
Relevance: Actionable PMF framework with tactical guidance on validation techniques and prioritization.

**L3. 俞军. (2020). *俞军产品方法论* (Yu Jun's Product Methodology).**
从用户价值角度建立产品决策框架，强调用户交易模型与长期用户价值最大化。
Relevance: Chinese PM philosophy emphasizing user value creation over feature delivery; critical for understanding Asian product markets.
```

**Minimum**: ≥6 entries (must include ≥2 ZH sources: 俞军, 梁宁, 苏杰, or equivalent)

---

#### D.4 APA Style Source Citations

**Format** (APA 7th edition with language tags):
```markdown
**A#. [Full APA citation]. [Language Tag]**
```

**Examples**:
```markdown
**A1. Cagan, M. (2017). *Inspired: How to create tech products customers love* (2nd ed.). Wiley. [EN]**

**A2. Olsen, D. (2015). *The lean product playbook: How to innovate with minimum viable products and rapid customer feedback*. Wiley. [EN]**

**A3. Torres, T. (2021). *Continuous discovery habits: Discover products that create customer value and business value*. Product Talk LLC. [EN]**

**A4. 俞军. (2020). *俞军产品方法论*. 中信出版社. [ZH]**
(Yu, J. (2020). *Yu Jun's product methodology*. CITIC Press.)

**A5. 梁宁. (2019). *产品思维30讲*. 得到App. [ZH]**
(Liang, N. (2019). *30 lessons on product thinking*. Dedao App.)

**A6. Klement, A. (2016). *When coffee and kale compete: Become great at making products people will buy*. Alan Klement. [EN]**

**A7. Mixpanel. (2024). *Product analytics glossary*. Mixpanel, Inc. https://mixpanel.com/topics/product-analytics-glossary [EN]**
```

**Requirements**:
- Minimum: ≥12 entries
- Language distribution: ~60% [EN], ~30% [ZH], ~10% [Other]
- Sort by ID (A1, A2, A3...)
- Include romanized transliteration for non-Latin scripts

---

### E. Validation Report Output Template

**Instructions**: Copy the validation report table from Section IV.B and fill in all measurements. Include this as final section of Q&A bank.

```markdown
## Validation Report

**Validation Date**: [YYYY-MM-DD]
**Validated By**: [Name/Role]

[Insert completed 12-step validation table from Section IV.B with all "___" placeholders filled in with actual measurements]

**Outcome**: [ALL PASS / FAIL - see issues below]

**Issues Found** *(if any)*:
- [List specific failures and corrective actions taken]

**Final Status**: [READY FOR USE / REQUIRES REVISION]
```

---

## VII. Complete Example Question

**Purpose**: Reference example demonstrating all template requirements.

### Q1: How would you evaluate whether to build a new feature requested by your top 5 enterprise customers that represents 40% of revenue, but doesn't align with your product vision for the mass market?

**Difficulty**: Advanced
**Topic Area**: Strategy & Vision, Prioritization & Roadmapping

**Key Insight**: Tests tension between short-term revenue protection (enterprise retention) and long-term product-market fit (mass-market scalability); distinguishes PMs who navigate executive pressure from those defaulting to either pleasing customers or rigid vision adherence.

---

**Answer**:

Use multi-dimensional evaluation [Ref: A1]. **First, discover the job-to-be-done** [Ref: A7]: what problem are customers solving? Surface requests often mask deeper needs revealing generalized solutions [Ref: A6].

**Second, quantify with RICE** [Ref: G2]. Enterprise: Reach (5/$2M), Impact (high retention/low acquisition), Confidence (high), Effort (unknown if custom). Mass-market: Reach (5K+ users), Impact (med/user, high cumulative), Confidence (med), Effort (similar). RICE won't capture strategic value—use decision matrix [Ref: T2].

**Third, assess against North Star Metric** [Ref: G4]. Does this move us toward outcomes or become feature factory [Ref: G9]? If we generalize (e.g., "custom fields" → "flexible schema"), both segments benefit and we strengthen PMF [Ref: G5].

**Finally, explore options**: (1) Generalized version; (2) Premium tier; (3) Professional services; (4) Partner ecosystem. Document precedent—product principles matter [Ref: L3]. Present analysis with clear recommendation, trade-offs, and decision criteria to stakeholders [Ref: T2].

**Supporting Artifact**:

```
Decision Matrix: Enterprise Request vs. Mass Market Feature

| Criterion                  | Enterprise Feature      | Mass Market Feature      | Weight | Score (E) | Score (M) |
|----------------------------|-------------------------|--------------------------|--------|-----------|-----------|
| Revenue impact (12mo)      | $2M (40% retention)     | $500K (new acquisition)  | 30%    | 9         | 3         |
| Strategic alignment        | Low (custom solution)   | High (vision-aligned)    | 25%    | 2         | 9         |
| Reach (users affected)     | 5 customers             | 5,000+ potential users   | 20%    | 1         | 9         |
| Product velocity impact    | High (custom complex)   | Low (reusable components)| 15%    | 2         | 8         |
| Competitive moat           | Low (replicable)        | High (differentiator)    | 10%    | 3         | 9         |
| **Weighted Score**         |                         |                          |        | **4.8**   | **7.1**   |

Recommendation: Pursue mass market feature + offer enterprise customers premium services engagement
```

---
