# Enterprise Risk Assessment Framework

Generate comprehensive risk assessment covering all risk dimensions across software system lifecycle with evidence-based analysis, mitigation strategies, and quantitative validation.

## I. Context & Scope

**Purpose**: Systematic risk identification, assessment, and mitigation planning across technical, business, regulatory, ecosystem, and financial dimensions throughout system lifecycle (requirements → evolution) for all stakeholders.

**Assumptions**: Production-grade distributed systems (>10K rps, >1TB data, multi-team); cloud-native polyglot environments; regulated/high-stakes domains; user provides context or accepts generic scenarios; comprehensive risk register output.

**Constraints**: 200-400 words/risk analysis (excluding artifacts); ≥80% risk coverage with mitigation strategies; 100% scenario-based (contextualized to lifecycle phase); quantitative probability/impact scoring required.

**Terms**: Risk (potential event impacting objectives), Probability (likelihood: 1-5), Impact (consequence severity: 1-5), Risk Score (Probability × Impact), Mitigation (preventive/corrective actions), Residual Risk (post-mitigation), Quality gate (mandatory validation checkpoint).

**Scope**: Technical debt, market dynamics, regulatory compliance, vendor dependencies, financial constraints, security threats, operational failures. **Exclude**: Force majeure, theoretical academic risks with no practical precedent.

**Limitations**: Generic risks require industry contextualization; mitigation strategies need resource/timeline adaptation; probability/impact scoring subjective without historical data; cross-risk dependencies may compound impact.

## II. Requirements

### Quantitative Floors

**Risk Coverage**: 40–60 total risks | Distributed across 5 dimensions × 8 lifecycle phases | 200–400 words/analysis | ≥80% with mitigation strategies | ≥60% with quantitative metrics | Each risk addresses stakeholder impact

**Risk Dimension Coverage (MECE)**:
1. **Technical** (8–12 risks): Architecture, infrastructure, code quality, security, data, performance, integration, operations
2. **Business/Market** (8–12 risks): Competition, market timing, product-market fit, pricing, customer adoption, revenue, partnerships
3. **Regulatory/Legal** (6–10 risks): Compliance (GDPR, HIPAA, SOC 2), licensing, data privacy, contractual obligations, IP, audit
4. **Ecosystem** (6–10 risks): Vendor lock-in, dependencies, platform changes, supply chain, community, standards
5. **Financial** (6–10 risks): Budget overruns, TCO, opportunity cost, funding, burn rate, ROI, debt accumulation

**Lifecycle Phase Coverage**: Each phase (Requirements → Evolution) must have ≥3 risks identified across ≥3 dimensions

**Stakeholder Coverage**: Each stakeholder role (BA, PM, Architect, Dev, QA, DevOps, Security, Data, SRE, Leadership) must be primary owner for ≥2 risks

**References** (build before risk analysis): Glossary ≥15 terms | Frameworks ≥8 (STRIDE, FAIR, ISO 31000, NIST, COBIT, etc.) | Tools ≥8 platforms | Literature ≥10 sources (≥3 risk management, ≥3 industry-specific) | Citations ≥20 APA 7th with [EN]/[ZH] tags

**Visuals**: ≥1 risk matrix + ≥1 mitigation roadmap per dimension (5+5 minimum) | ≥1 heat map across lifecycle

**Scaling**: For >60 risks, multiply reference floors by 1.5×

### Citation Standards

**Format**: APA 7th + tag: `Author, A. (Year). *Title*. Publisher. [EN]` | Inline: `[Ref: ID]` (G#=Glossary, F#=Framework, T#=Tool, L#=Literature, C#=Citation)

**Distribution**: EN 50–70% (target 60%) | ZH 20–40% (target 30%) | Other 5–15% (target 10%)

**Source Types** (≥4): Risk frameworks (ISO 31000, FAIR, NIST, COBIT), Industry research/data (Gartner, Forrester, IDC), Case studies (incidents, postmortems, breach reports), Risk management tools (ServiceNow, Archer, LogicGate), Compliance standards (GDPR, HIPAA, SOC 2, PCI-DSS)

### Quality Gates (fail ANY = stop, fix, re-validate ALL)

1. **Recency**: ≥60% from last 3yrs (≥80% for cybersecurity, cloud, fintech, healthcare)
2. **Source Diversity**: ≥4 types; no type >30%
3. **Per-Dimension Coverage**: Each dimension has ≥3 authoritative sources + ≥2 frameworks + ≥1 tool
4. **Per-Phase Coverage**: Each lifecycle phase has ≥3 risks with evidence-based mitigation
5. **Stakeholder Validation**: 100% risks assigned to owner + escalation path
6. **Quantitative Scoring**: ≥80% risks have probability/impact scores + financial/time estimates
7. **Mitigation Completeness**: ≥80% high-severity risks (score ≥15) have 3-tier mitigation (preventive/detective/corrective)
8. **Tool Completeness**: Risk management tools include features, integrations, compliance support
9. **Link Validation**: 100% URLs accessible/archived
10. **Cross-Reference Integrity**: 100% [Ref: ID] resolve; no orphans
11. **Dependency Mapping**: Cross-risk dependencies identified and documented
12. **Residual Risk**: All mitigations include residual risk assessment

**Mitigation**: Recency fail → flag dated standards with sunset dates | Diversity fail → expand to adjacent domains | Scoring fail → use industry benchmarks | Dependency fail → create risk dependency graph

## III. Execution

### Step 1: Plan Risk Allocation

Distribute 40–60 risks across 5 dimensions × 8 lifecycle phases. Target: 5–7 risks per dimension, 3–8 risks per phase.

**Example** (50 risks):
- Technical: 10 risks (Requirements: 2, Design: 2, Development: 2, Testing: 1, Deployment: 1, Operations: 1, Maintenance: 1, Evolution: 0)
- Business/Market: 10 risks (Requirements: 3, Design: 1, Development: 1, Testing: 1, Deployment: 1, Operations: 1, Maintenance: 1, Evolution: 1)
- Regulatory/Legal: 8 risks (Requirements: 2, Design: 2, Development: 1, Testing: 1, Deployment: 1, Operations: 0, Maintenance: 1, Evolution: 0)
- Ecosystem: 8 risks (Requirements: 1, Design: 2, Development: 2, Testing: 1, Deployment: 1, Operations: 0, Maintenance: 0, Evolution: 1)
- Financial: 8 risks (Requirements: 2, Design: 1, Development: 1, Testing: 1, Deployment: 1, Operations: 1, Maintenance: 1, Evolution: 0)

**Severity Distribution Target**: Critical (score ≥20): 10–15% | High (15–19): 25–30% | Medium (10–14): 40–45% | Low (5–9): 15–20%

### Step 2: Build References (BEFORE risk analysis → run Gates 1–12 after)

**Glossary (≥15)**: Risk, Threat, Vulnerability, Impact, Probability, Mitigation, Residual Risk, Risk Appetite, Risk Tolerance, SLA, SLO, SLI, MTTR, MTBF, RTO, RPO, CVSS, CVE, STRIDE, DREAD, FAIR, ISO 31000, NIST CSF, COBIT, TCO, ROI, Error Budget | Format: term, definition, formula/scale (if applicable), usage context, related terms | Assign G1, G2...

**Frameworks (≥8)**: ISO 31000 (risk management), FAIR (financial risk quantification), NIST Cybersecurity Framework, STRIDE (threat modeling), DREAD (risk rating), COBIT (IT governance), TOGAF (architecture risk), OWASP Top 10 (security), CIS Controls (security), DORA Metrics (DevOps), SRE principles (reliability) | Include: purpose, components, application, limitations, references | Assign F1, F2...

**Tools (≥8)**: Risk Management (ServiceNow GRC, Archer, LogicGate, RiskLens), Security (Snyk, SonarQube, Checkmarx, Wiz), Monitoring (Prometheus, Grafana, Datadog, New Relic), Compliance (Vanta, Drata, OneTrust), Incident (PagerDuty, Opsgenie) | Include: category, features, pricing, integrations, compliance support, update date, URL | Assign T1, T2...

**Literature (≥10)**: EN—*Site Reliability Engineering* (Beyer), *Security Engineering* (Anderson), *Release It!* (Nygard), *The Phoenix Project* (Kim), *Accelerate* (Forsgren), ISO 31000:2018 standard, NIST SP 800-30 | ZH (≥3)—风险管理标准, 信息安全管理, 合规性指南 | Include: author, title, year, key concepts, applicability | Assign L1, L2...

**Citations (≥20)**: Convert to APA 7th + tags | Verify ≥60% from last 3yrs | Classify: frameworks/standards/research/case studies/breach reports/tools | Assign C1, C2... | **Alternatives**: Gartner Risk Research, Forrester Security Reports, SANS Institute, OWASP, NIST Publications, ISO Standards, Verizon DBIR, Cloud Security Alliance

### Step 3: Generate Risk Assessments (5–10 at a time → self-check each batch)

**Risk Identification**: Scenario-based contextualized to lifecycle phase | Name format: "[Dimension]-[Phase]-[Specific Risk]" (e.g., "Technical-Development-UntestedDatabaseMigration") | Root cause analysis | Triggering conditions | Affected stakeholders

**Risk Analysis** (200–400 words):
1. **Risk Statement** (1–2 sentences): Clear description of event and consequence
2. **Lifecycle Phase**: When risk manifests [Ref: Handbook phases]
3. **Dimension**: Primary dimension (Technical/Business/Regulatory/Ecosystem/Financial) + secondary if applicable
4. **Probability** (1–5): 1=Rare (<10%), 2=Unlikely (10–25%), 3=Possible (25–50%), 4=Likely (50–75%), 5=Almost Certain (>75%) [Ref: F#]
5. **Impact** (1–5): 1=Negligible, 2=Minor, 3=Moderate, 4=Major, 5=Catastrophic (define in financial, time, reputation terms) [Ref: F#]
6. **Risk Score**: Probability × Impact (range 1–25)
7. **Stakeholder Impact**: Primary owner, affected teams, escalation path
8. **Root Causes**: Technical/process/organizational factors [Ref: G#/C#]
9. **Triggers & Indicators**: Leading indicators, monitoring metrics [Ref: T#]
10. **Dependencies**: Related risks, cascading effects
11. **Mitigation Strategy**:
    - **Preventive**: Actions to reduce probability (e.g., architecture review, automated testing)
    - **Detective**: Monitoring/alerting to catch early (e.g., dashboards, anomaly detection) [Ref: T#]
    - **Corrective**: Response plan if occurs (e.g., rollback, DR activation) [Ref: L#]
    - **Timeline**: Implementation phases with milestones
    - **Cost Estimate**: Resources, tools, time investment
12. **Residual Risk**: Post-mitigation probability/impact scores
13. **Success Metrics**: KPIs to validate mitigation effectiveness
14. **Citations**: ≥2 [Ref: ID] for high-severity risks, ≥1 for medium/low
15. **Artifact** (required for Critical/High risks): Risk matrix, mitigation roadmap, decision tree, runbook

**Batch Self-Check** (per 5–10 risks): 
- Scenario-based contextualized to phase
- 200–400 words
- Probability/Impact scored with justification
- Stakeholder owner assigned
- ≥80% have mitigation strategy (preventive + detective + corrective for high-severity)
- ≥4/5 have ≥1 citation
- Financial/time estimates for high-severity risks
- Residual risk calculated

### Step 4: Create Visuals (≥1 risk matrix + ≥1 mitigation roadmap per dimension; ≥1 lifecycle heat map)

**Risk Matrices (5×5 grid per dimension)**: Plot risks by Probability (Y-axis, 1–5) × Impact (X-axis, 1–5) | Color zones: Green (1–6), Yellow (8–12), Orange (15–16), Red (≥20) | Label each risk with ID

**Mitigation Roadmaps (per dimension)**: Timeline chart (Gantt-style) showing preventive/detective/corrective actions with owners, milestones, dependencies | Include Quick Wins (low-effort, high-impact), Strategic Initiatives (high-effort, high-impact), Backlog (low priority)

**Lifecycle Heat Map (cross-dimension)**: 8 phases (columns) × 5 dimensions (rows) | Cell color intensity = aggregated risk score for that phase-dimension combination | Include risk count per cell

**Additional Artifacts**:
- **Technical**: Architecture threat model (STRIDE), dependency graph, failure mode tree
- **Business/Market**: Competitive landscape, market timing chart, adoption funnel with risk points
- **Regulatory/Legal**: Compliance matrix (requirements × controls), audit timeline, data flow diagram with PII markers
- **Ecosystem**: Vendor dependency map, integration risk matrix, platform roadmap alignment
- **Financial**: Cost breakdown (OpEx/CapEx), burn rate projection with risk scenarios, ROI sensitivity analysis

**Risk Dependency Graph**: Network diagram showing cascading risks and amplification effects

**Best Practices**: Use consistent color coding (Red/Orange/Yellow/Green) | Include severity scores | Show mitigation status (Not Started/In Progress/Complete) | Cite data sources | Include legends and timestamps

### Step 5: Populate References

**Glossary**: **G#. Term (Acronym)** | Definition (with scale/formula if applicable) | Usage context | Related terms | Limitations | Alphabetize | **Example**: G1. Risk Score | Probability × Impact (range 1–25) | Used in risk prioritization | Related: Probability, Impact, Severity | Limitations: Subjective scoring without historical data

**Frameworks**: **F#. Framework Name** | Purpose | Key components | Application methodology | Limitations | Source citation | **Example**: F1. ISO 31000:2018 | Enterprise risk management | Principles, framework, process | Apply across lifecycle phases | Generic—requires contextualization | ISO (2018)

**Tools**: **T#. Tool (Category)** | Description | Features (risk-specific) | Pricing tier | Integrations (≥3) | Compliance support (GDPR, SOC 2, etc.) | Update (Q# YYYY) | Risk management use case | URL | Group by category | **Example**: T1. ServiceNow GRC (Risk Management) | Enterprise risk & compliance platform | Risk register, heat maps, workflow automation, audit trails | Enterprise: $100+/user/mo | Jira, Slack, Azure AD, Okta | GDPR, SOC 2, HIPAA, ISO 27001 | Q4 2024 | Centralized risk tracking with stakeholder RACI | URL

**Literature**: **L#. Author, Title, Year** | Summary (key risk concepts, frameworks, case studies) | Applicability (industry, phase, dimension) | Relevance to distributed systems | Group by language (EN first, then ≥3 ZH) | **Example**: L1. Beyer, B. et al. (2016). *Site Reliability Engineering*. O'Reilly. | SLO/SLI/error budgets, incident response, capacity planning | Applicable: Operations/Maintenance phases, Technical/Financial dimensions | Core SRE principles for production systems | [EN]

**Citations**: **C#. [Citation] [Lang]** | Standards: `Organization. (Year). *Standard ID: Title*. [EN]` | Books: `Author, A. (Year). *Title*. Publisher. [EN]` | Articles: `Author, A. (Year). Title. *Journal*, Vol(Issue), pages. DOI [EN]` | Reports: `Organization. (Year). *Report Title*. URL [EN]` | ZH: `作者. (年份). *标题*. 出版社. [ZH] (Author (Year). *Title*. Publisher.)` | Sort by ID | **Example**: C1. NIST. (2012). *SP 800-30: Guide for Conducting Risk Assessments*. https://doi.org/10.6028/NIST.SP.800-30r1 [EN]

**Check**: 100% [Ref: ID] resolve | No orphans | All fields complete | All citations have APA format + language tags | All tools have compliance support documented

### Step 6: Run 18 Validations (fail ANY = stop, fix, re-run ALL)

1. **Floors**: G≥15, F≥8, T≥8, L≥10, C≥20, Risks=40–60
2. **Dimension Coverage**: 5/5 dimensions have ≥5 risks; distribution 8–12 per dimension (±2)
3. **Phase Coverage**: 8/8 phases have ≥3 risks across ≥3 dimensions
4. **Severity Distribution**: Critical (≥20): 10–15% | High (15–19): 25–30% | Medium (10–14): 40–45% | Low (5–9): 15–20%
5. **Stakeholder Assignment**: 100% risks have owner + escalation path; each stakeholder owns ≥2 risks
6. **Mitigation Coverage**: ≥80% risks have mitigation strategies; ≥80% high-severity (score ≥15) have 3-tier mitigation (preventive + detective + corrective)
7. **Quantitative Scoring**: 100% risks have probability/impact scores with justification; ≥60% have financial/time estimates
8. **Residual Risk**: 100% mitigated risks have residual probability/impact documented
9. **Citations**: ≥80% risks have ≥1 citation; ≥60% high-severity have ≥2
10. **Language Distribution**: EN 50–70%, ZH 20–40%, Other 5–15%
11. **Recency**: ≥60% from last 3yrs (≥80% for cybersecurity/cloud/fintech/healthcare)
12. **Source Diversity**: ≥4 types (frameworks/research/case studies/tools/standards); no type >30%
13. **Links**: 100% URLs accessible/archived
14. **Cross-References**: 100% [Ref: ID] resolve; no orphans
15. **Word Count**: Sample 10 risks; 100% within 200–400 words
16. **Framework Application**: ≥80% risks correctly apply frameworks + cite + document limitations
17. **Dependency Mapping**: Risk dependency graph includes all cascading risks; ≥30% risks have documented dependencies
18. **Artifact Completeness**: 100% Critical/High risks have visual artifacts; ≥50% Medium risks have artifacts

### Step 7: Final Review

**Risk Quality** (sample ≥10 risks across dimensions/phases): 
- **Clarity**: Risk statement is concrete, measurable, and contextualized to phase
- **Relevance**: Real-world risk with documented precedent (case studies, breach reports, postmortems)
- **Completeness**: All 15 risk analysis components present (statement, phase, dimension, probability, impact, score, stakeholder, root causes, triggers, dependencies, mitigation 3-tier, residual risk, metrics, citations, artifacts)
- **Actionability**: Mitigation strategies are specific, costed, and time-bound with owners
- **Quantification**: Probability/impact scores justified with data or industry benchmarks
- **Stakeholder Alignment**: Owner has authority/capability to mitigate; escalation path is appropriate

**Cross-Dimension Balance**: Review risk distribution to ensure no over/under-representation; validate that technical risks don't dominate at expense of business/regulatory/ecosystem/financial

**Lifecycle Realism**: Risks contextualized to phase maturity (e.g., Requirements phase focuses on scoping/NFR risks, not production incident risks)

**Mitigation Feasibility**: Strategies are implementable given constraints (budget, team size, timeline, tech stack); no "solution by committee" or unrealistic dependencies

**Stakeholder Review Checklist**:
- **Leadership**: High-severity risks (≥15) escalation paths correct; financial estimates reasonable; risk appetite alignment
- **Architect**: Technical risks architecturally sound; mitigation doesn't introduce greater risk
- **Security**: Threat models complete; compliance risks have regulatory mapping; residual risks acceptable
- **SRE**: Operational risks include monitoring/alerting; MTTR/RTO targets realistic
- **DevOps**: Deployment risks have rollback plans; pipeline security validated
- **PM**: Business risks prioritized correctly; market timing analyzed; competitive threats assessed
- **Finance**: Cost estimates include TCO (not just CapEx); ROI assumptions documented

**Submission Checklist**: 
- All 18 validations PASS
- All floors met (G≥15, F≥8, T≥8, L≥10, C≥20, Risks=40–60)
- All 12 quality gates passed
- TOC with risk taxonomy
- No placeholders or TBDs
- Consistent formatting (risk IDs, probability/impact scales, color coding)
- Balanced coverage across dimensions/phases/stakeholders
- Risk register exportable (CSV/JSON) with all fields
- Heat maps and matrices include timestamps
- Executive summary with top 10 risks + mitigation priorities

## IV. Validation Report (fill all; ANY fail = stop, fix, re-run ALL)

| # | Check                    | Measurement                                    | Criteria                                      | Result | Status    |
|---|--------------------------|------------------------------------------------|-----------------------------------------------|--------|-----------|
| 1 | Floors                   | G:__ F:__ T:__ L:__ C:__ Risks:__              | G≥15, F≥8, T≥8, L≥10, C≥20, Risks:40-60       |        | PASS/FAIL |
| 2 | Dimension Coverage       | Tech:__ Biz:__ Reg:__ Eco:__ Fin:__            | Each dimension: 8-12 risks (±2)               |        | PASS/FAIL |
| 3 | Phase Coverage           | __/8 phases have ≥3 risks across ≥3 dimensions | 8/8                                           |        | PASS/FAIL |
| 4 | Severity Distribution    | Crit:__% High:__% Med:__% Low:__%              | Crit:10-15%, High:25-30%, Med:40-45%, Low:15-20% |     | PASS/FAIL |
| 5 | Stakeholder Assignment   | __/__ risks have owner+escalation              | 100%; each stakeholder owns ≥2                |        | PASS/FAIL |
| 6 | Mitigation Coverage      | __% have strategy; __% high-severity 3-tier    | ≥80% strategy; ≥80% high 3-tier               |        | PASS/FAIL |
| 7 | Quantitative Scoring     | __% have P/I scores; __% have estimates        | 100% scores; ≥60% estimates                   |        | PASS/FAIL |
| 8 | Residual Risk            | __/__ mitigated risks have residual            | 100%                                          |        | PASS/FAIL |
| 9 | Citations                | __%≥1, __%≥2 (high-severity)                   | ≥80%≥1, ≥60% high≥2                           |        | PASS/FAIL |
| 10| Language Distribution    | EN:__%, ZH:__%, Other:__%                      | EN:50-70%, ZH:20-40%, Other:5-15%             |        | PASS/FAIL |
| 11| Recency                  | __% from 3yrs (domain: ___)                    | ≥60% (≥80% cyber/cloud/fintech/health)        |        | PASS/FAIL |
| 12| Source Diversity         | __ types; max __%                              | ≥4 types, max 30%                             |        | PASS/FAIL |
| 13| Links                    | __/__ accessible                               | 100%                                          |        | PASS/FAIL |
| 14| Cross-References         | __/__ resolved                                 | 100%                                          |        | PASS/FAIL |
| 15| Word Count               | __ sampled: __ compliant                       | 100% (200-400)                                |        | PASS/FAIL |
| 16| Framework Application    | __/__ correct+cited+limits                     | ≥80%                                          |        | PASS/FAIL |
| 17| Dependency Mapping       | __% risks have dependencies documented         | ≥30%                                          |        | PASS/FAIL |
| 18| Artifact Completeness    | Crit/High:__% Med:__%                          | Crit/High:100%, Med:≥50%                      |        | PASS/FAIL |

## V. Risk Quality (review each; fails ≥2 criteria = revise/replace)

1. **Clarity**: Specific, measurable event + consequence | ✓ "Database migration fails during deployment causing 4hr+ downtime" | ✗ "System might have issues"

2. **Contextualization**: Tied to specific lifecycle phase | ✓ "Requirements phase: Incomplete NFRs lead to architecture redesign in Development phase (3mo delay, $500K)" | ✗ "Project delays happen"

3. **Quantification**: Probability/Impact scored with justification | ✓ "Probability=4 (60% of microservices migrations encounter schema conflicts—DORA 2023); Impact=4 ($800K labor + 8wk delay + customer churn)" | ✗ "High risk"

4. **Actionability**: Mitigation is specific, costed, owned | ✓ "Preventive: Schema versioning + test migration in staging ($20K tool, 2wk, DevOps lead). Detective: Migration health dashboard (Grafana). Corrective: Rollback automation (1hr RTO)" | ✗ "Test more thoroughly"

5. **Realism**: Based on real-world precedent | ✓ "GitLab 2017 database incident (6hr data loss, 5K user impact) demonstrates risk severity [Ref: C12]" | ✗ "Theoretical worst-case scenario with no historical basis"

6. **Stakeholder Alignment**: Owner has authority/capability | ✓ "Owner: DevOps Lead (has budget authority + staging environment access). Escalation: CTO if RTO >2hr or revenue-impacting" | ✗ "Owner: Junior developer. Escalation: unspecified"

7. **Completeness**: All 15 components present | ✓ Risk has statement, phase, dimension, P/I scores, stakeholder, root causes, triggers, dependencies, 3-tier mitigation, residual risk, metrics, ≥1 citation, artifact | ✗ Missing mitigation strategy or residual risk

8. **Dimension Appropriateness**: Risk correctly categorized | ✓ Technical risk: Architecture coupling causing deployment failures | ✗ Calling "budget overrun" a technical risk (should be Financial)

## VI. Output Format

### A. TOC
1. Executive Summary (Top 10 risks + mitigation priorities) | 2. Risk Taxonomy (5 dimensions × 8 phases matrix) | 3. Risk Register (full catalog) | 4. Mitigation Roadmap | 5. References (Glossary, Frameworks, Tools, Literature, Citations) | 6. Validation Report | 7. Appendices (Heat maps, dependency graph, stakeholder RACI)

### B. Risk Taxonomy Overview
**Total Risks**: [40–60] | **Severity**: Critical [X] ([Y]%) / High [X] ([Y]%) / Medium [X] ([Y]%) / Low [X] ([Y]%) | **Coverage**: 5 dimensions × 8 lifecycle phases (MECE)

| # | Dimension          | Risk Count | Severity Mix (C/H/M/L) | Artifacts                                    |
|---|--------------------|------------|-------------------------|----------------------------------------------|
| 1 | Technical          | 8–12       | 2C/3H/4M/2L             | Risk matrix + Threat model + Mitigation roadmap |
| 2 | Business/Market    | 8–12       | 1C/3H/5M/2L             | Risk matrix + Competitive analysis + Mitigation roadmap |
| 3 | Regulatory/Legal   | 6–10       | 1C/2H/4M/1L             | Risk matrix + Compliance matrix + Mitigation roadmap |
| 4 | Ecosystem          | 6–10       | 1C/2H/4M/2L             | Risk matrix + Dependency map + Mitigation roadmap |
| 5 | Financial          | 6–10       | 1C/3H/3M/2L             | Risk matrix + Cost sensitivity + Mitigation roadmap |
|   | **Total**          | **50**     | **7C/13H/20M/9L**       | **5 matrices + 5 roadmaps + heat map + dependency graph** |

Legend: C=Critical (≥20) | H=High (15–19) | M=Medium (10–14) | L=Low (5–9)

### C. Lifecycle Phase Distribution

| Phase                       | Tech | Biz | Reg | Eco | Fin | **Total** | **Top Risk Score** |
|-----------------------------|------|-----|-----|-----|-----|-----------|---------------------|
| 1. Requirements & Discovery | 2    | 3   | 2   | 1   | 2   | **10**    | 20 (Crit)           |
| 2. Architecture & Design    | 2    | 1   | 2   | 2   | 1   | **8**     | 18 (High)           |
| 3. Development              | 2    | 1   | 1   | 2   | 1   | **7**     | 16 (High)           |
| 4. Testing & Quality        | 1    | 1   | 1   | 1   | 1   | **5**     | 15 (High)           |
| 5. Deployment & Release     | 1    | 1   | 1   | 1   | 1   | **5**     | 20 (Crit)           |
| 6. Operations & Observability | 1  | 1   | 0   | 0   | 1   | **3**     | 16 (High)           |
| 7. Maintenance & Support    | 1    | 1   | 1   | 0   | 1   | **4**     | 12 (Med)            |
| 8. Evolution & Governance   | 0    | 1   | 0   | 1   | 0   | **2**     | 12 (Med)            |
| **Total**                   | **10** | **10** | **8** | **8** | **8** | **50** | - |

### D. Risk Entry Format

**Dimension: [Technical/Business/Regulatory/Ecosystem/Financial]**

**RISK-[ID]: [Dimension]-[Phase]-[SpecificRisk]**

**Risk Statement**: [1–2 sentences describing event and consequence]

**Lifecycle Phase**: [Requirements/Design/Development/Testing/Deployment/Operations/Maintenance/Evolution]

**Probability** (1–5): [Score] | [Justification with data/precedent] [Ref: F#/C#]

**Impact** (1–5): [Score] | Financial: [$X loss/cost], Time: [X wks delay], Reputation: [customer churn, SLA breach, regulatory penalty]

**Risk Score**: [Probability × Impact] | Severity: [Critical/High/Medium/Low]

**Stakeholder Impact**:
- **Owner**: [Role with authority/capability]
- **Affected Teams**: [List]
- **Escalation Path**: [Conditions → Role]

**Root Causes**: [Technical/process/organizational factors] [Ref: G#/C#]

**Triggers & Indicators**: 
- **Leading Indicators**: [Metrics to monitor before risk manifests]
- **Monitoring**: [Tools, dashboards, alerts] [Ref: T#]

**Dependencies**: [Related risks, cascading effects] | Cross-refs: [RISK-IDs]

**Mitigation Strategy** (200–400 words total):

1. **Preventive** (reduce probability):
   - Actions: [Specific steps]
   - Owner: [Role]
   - Timeline: [X weeks/months]
   - Cost: [$X or X person-hours]
   - Tools: [Ref: T#]

2. **Detective** (early detection):
   - Monitoring: [Dashboards, alerts, thresholds]
   - Tools: [Ref: T#]
   - SLA: [Detection time target]

3. **Corrective** (response plan):
   - Actions: [Rollback, DR, failover steps]
   - Owner: [Role]
   - RTO/RPO: [Time targets]
   - Runbook: [Link or ref]
   - Tools: [Ref: T#]

**Residual Risk**:
- Post-mitigation Probability: [Score]
- Post-mitigation Impact: [Score]
- Residual Score: [P × I]
- Acceptance: [Why residual risk is acceptable, or escalation if not]

**Success Metrics**: [KPIs to validate mitigation effectiveness]

**Citations**: [Ref: C#, C#, ...] (≥2 for Critical/High, ≥1 for Medium/Low)

**Artifact** *(required for Critical/High)*: [Risk matrix position, mitigation timeline, decision tree, runbook, monitoring dashboard]

---

**Example Entry**:

**RISK-T-DEV-01: Technical-Development-UntestedDatabaseMigration**

**Risk Statement**: Database schema migration deployed to production without comprehensive testing in staging environment, causing data inconsistency, application errors, and extended downtime (4–8 hours) requiring rollback or emergency hotfix.

**Lifecycle Phase**: Development

**Probability** (1–5): 4 | Justification: 60% of organizations experience migration-related incidents in microservices architectures within first year (DORA State of DevOps 2023); team lacks automated migration testing [Ref: C15]

**Impact** (1–5): 4 | Financial: $800K (labor $200K + revenue loss $500K/hr × 4hr + customer churn $100K), Time: 8 weeks recovery effort, Reputation: SLA breach (99.9% → 99.5%), 3 enterprise customer escalations

**Risk Score**: 16 | Severity: High

**Stakeholder Impact**:
- **Owner**: DevOps Lead (has staging environment authority, deployment pipeline control, $50K discretionary budget)
- **Affected Teams**: Backend developers (schema changes), QA (test data), SRE (incident response), Customer Success (escalations)
- **Escalation Path**: If RTO >2hr or revenue-impacting → CTO; if SLA breach → CEO + affected customers

**Root Causes**: [Ref: G8-Technical Debt] Insufficient test coverage for migrations; lack of schema versioning strategy; production-staging environment drift; manual deployment process susceptible to human error

**Triggers & Indicators**:
- **Leading Indicators**: Rising migration test failures in CI; schema complexity (# tables/foreign keys) increasing >20%/quarter; staging-production config drift
- **Monitoring**: Schema migration health dashboard (Grafana) [Ref: T5]; error rate spike (>1% sustained 5min) [Ref: T6]; database connection pool exhaustion

**Dependencies**: Cross-refs: RISK-T-DEP-02 (deployment rollback failure amplifies impact), RISK-B-OPS-01 (customer churn accelerates if incident during peak usage)

**Mitigation Strategy**:

1. **Preventive** (reduce probability to 2):
   - Actions: (1) Implement Liquibase/Flyway schema versioning [Ref: T3]; (2) automated migration testing in CI with production-like data volume (10K+ rows); (3) staging environment parity validation (weekly drift detection); (4) peer review requirement for all schema changes
   - Owner: DevOps Lead (implementation) + Backend Tech Lead (review process)
   - Timeline: 6 weeks (2wk tool setup + 2wk test harness + 2wk process rollout)
   - Cost: $25K (Liquibase enterprise license $15K/yr + 80 person-hours $10K)
   - Tools: [Ref: T3-Liquibase, T4-Testcontainers]

2. **Detective** (early detection, 5min SLA):
   - Monitoring: Migration health dashboard (pre-migration checks, execution progress, post-migration validation) [Ref: T5-Grafana]; alert on error rate >0.5% or latency p95 >500ms (baseline 200ms)
   - Tools: [Ref: T5-Grafana, T6-Prometheus]
   - SLA: Alert within 2min; incident declared within 5min

3. **Corrective** (RTO 1hr):
   - Actions: (1) Automated rollback playbook (database restore from pre-migration snapshot + application version revert); (2) migration hotfix process (expedited review + canary deployment); (3) communication template (internal stakeholders + affected customers)
   - Owner: SRE on-call (execution) + DevOps Lead (approval for hotfix)
   - RTO/RPO: 1hr RTO (30min detection + 30min rollback), 15min RPO (transaction log shipping interval)
   - Runbook: [Link: /runbooks/db-migration-incident.md]
   - Tools: [Ref: T7-PagerDuty incident management, T8-Automated backup/restore]

**Residual Risk**:
- Post-mitigation Probability: 2 (Unlikely, 10–25%)
- Post-mitigation Impact: 3 (Moderate, $200K + 2wk recovery)
- Residual Score: 6 (Low)
- Acceptance: Acceptable—residual risk within risk appetite (score <10); further reduction (e.g., blue-green database deployment) cost-prohibitive ($200K+ infrastructure) for current scale; revisit at 100K+ users

**Success Metrics**: Zero migration-related incidents in 12 months; 100% migrations pass staging validation; mean schema change cycle time <3 days (from PR to production)

**Citations**: 
- [Ref: C15] Forsgren, N., et al. (2023). *DORA State of DevOps Report*. Google Cloud. [EN]
- [Ref: C16] Kleppmann, M. (2017). *Designing Data-Intensive Applications*. O'Reilly. (Ch. 4: Schema Evolution) [EN]
- [Ref: C17] Richardson, C. (2018). *Microservices Patterns*. Manning. (Pattern: Database per Service) [EN]

**Artifact**: 
- Risk matrix position: Probability=4, Impact=4 (upper-right red zone)
- Mitigation timeline: [Gantt chart showing 6-week preventive implementation with milestones]
- Runbook: [Flowchart for incident detection → diagnosis → rollback decision → execution → validation → communication]

### E. Reference Formats

**Glossary**: **G#. Term (Acronym)** | Definition (with scale/formula if applicable) | Usage context | Related terms | Limitations | Alphabetize

**Frameworks**: **F#. Framework Name** | Purpose | Key components | Application methodology | Limitations | Source citation

**Tools**: **T#. Tool (Category)** | Description | Features (risk-specific) | Pricing tier | Integrations (≥3) | Compliance support | Update (Q# YYYY) | Risk management use case | URL | Group by category

**Literature**: **L#. Author, Title, Year** | Summary (key risk concepts, frameworks, case studies) | Applicability (industry, phase, dimension) | Relevance to distributed systems | Group by language (EN first, then ≥3 ZH)

**Citations**: **C#. [Citation] [Lang]** | Standards/Books/Articles/Reports with APA 7th format + language tags

## VII. Risk Assessment Template (Ready-to-Use)

When generating a risk assessment, use this structure:

```markdown
# Enterprise Risk Assessment: [Project/System Name]

**Assessment Date**: [YYYY-MM-DD]  
**Scope**: [System scope, e.g., "Payment processing microservices architecture"]  
**Team**: [Team size, structure]  
**Context**: [Key constraints: budget, timeline, regulatory requirements]

---

## Executive Summary

### Top 10 Risks (by severity score)

| Rank | Risk ID | Risk Name | Score | Dimension | Phase | Owner | Mitigation Priority |
|------|---------|-----------|-------|-----------|-------|-------|---------------------|
| 1    | RISK-T-DEP-01 | Deployment rollback failure | 20 (C) | Technical | Deployment | DevOps Lead | Immediate |
| 2    | RISK-B-REQ-01 | Market timing misalignment | 18 (H) | Business | Requirements | PM | High |
| ...  | ...         | ...                         | ...    | ...       | ...        | ...       | ...       |

### Mitigation Priorities

**Immediate (Week 1-4)**: [Critical risks requiring immediate action]  
**High Priority (Month 2-3)**: [High-severity risks]  
**Strategic (Quarter 2-4)**: [Medium-severity systemic risks]  
**Backlog**: [Low-severity risks, monitor only]

---

## Risk Taxonomy

[Insert 5×8 matrix showing dimension × phase distribution with risk counts and heat map]

---

## Risk Register

### Technical Risks

**RISK-T-DEV-01: Technical-Development-UntestedDatabaseMigration**

[Full risk entry following format from Section VI.D]

---

[Continue for all 40-60 risks organized by dimension]

---

## References

### Glossary

**G1. Risk Score** | Probability × Impact (range 1–25) | Used in risk prioritization | Related: Probability, Impact, Severity | Limitations: Subjective scoring without historical data

[Continue for ≥15 terms]

### Frameworks

**F1. ISO 31000:2018** | Enterprise risk management | Principles, framework, process | Apply across lifecycle phases | Generic—requires contextualization | ISO (2018)

[Continue for ≥8 frameworks]

### Tools

[≥8 tools with full specifications]

### Literature

[≥10 sources with ≥3 risk management focused]

### Citations

[≥20 citations in APA 7th format with language tags]

---

## Validation Report

[Insert completed 18-validation table from Section IV]

---

## Appendices

### A. Risk Matrices (by Dimension)

[5×5 grids for each of 5 dimensions]

### B. Lifecycle Heat Map

[8 phases × 5 dimensions with color-coded risk intensity]

### C. Mitigation Roadmap

[Gantt chart showing all mitigation activities with owners, timelines, dependencies]

### D. Risk Dependency Graph

[Network diagram showing cascading risks]

### E. Stakeholder RACI

[RACI matrix showing risk ownership across all roles]

```

---

## VIII. Quality Checklist (Before Submission)

✅ All 18 validations PASS  
✅ Executive summary with top 10 risks  
✅ Risk taxonomy table complete  
✅ All 40-60 risks have complete 15-component analysis  
✅ All Critical/High risks have visual artifacts  
✅ All risks have assigned owners + escalation paths  
✅ All high-severity risks have 3-tier mitigation (preventive/detective/corrective)  
✅ Mitigation roadmap with timeline + costs  
✅ References complete (G≥15, F≥8, T≥8, L≥10, C≥20)  
✅ All citations resolve with APA 7th format + language tags  
✅ Heat maps and dependency graph included  
✅ Stakeholder RACI matrix complete  
✅ No placeholders or TBDs  
✅ Consistent formatting (IDs, scales, colors)  
✅ Risk register exportable (CSV/JSON)  
✅ Document reviewed by ≥3 stakeholder roles
